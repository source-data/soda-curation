# Evaluation configuration file

# Global settings
output_dir: "/app/data/benchmark/"
ground_truth_dir: "/app/data/ground_truth"
manuscript_dir: "/app/data/archives"
prompts_source: "/app/config.dev.yaml"

# Test selection
enabled_tests:
  # - extract_sections
  - extract_individual_captions
  - assign_panel_source
  - extract_data_availability
  # - match_caption_panel

# Model configurations to test
providers:
  openai:
    models:
      - name: "gpt-4o"
        temperatures: [1.0, 0.5]
        top_p: [1.0]
      - name: "gpt-4o-mini"
        temperatures: [1.0, 0.5]
        top_p: [1.0]

# Test run configuration  
test_runs:
  n_runs: 3  # Number of times to run each configuration
  manuscripts: "all"

cache:
  version: "1.2.0"  # Increment this when making significant changes

# Results configuration
