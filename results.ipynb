{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark of SODA-curation\n",
    "\n",
    "This notebook will show the results of the benchmark of SODA-curation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preliminar functions and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import difflib\n",
    "from IPython.display import display, HTML\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import html\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Download necessary NLTK data if not already present\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_panel_seq(task, actual, expected, score):\n",
    "    if task == \"panel_sequence\":\n",
    "        # Convert string representations to actual lists\n",
    "        if isinstance(expected, str):\n",
    "            try:\n",
    "                # Safe way to convert string representation to list\n",
    "                expected = ast.literal_eval(expected)\n",
    "            except:\n",
    "                pass  # Keep as is if conversion fails\n",
    "                \n",
    "        if isinstance(actual, str):\n",
    "            try:\n",
    "                actual = ast.literal_eval(actual)\n",
    "            except:\n",
    "                pass  # Keep as is if conversion fails\n",
    "        \n",
    "        # Now proceed with the comparisons (on proper list objects)\n",
    "        if \"\".join(expected) == \"A\" and \"\".join(actual) == \"\":\n",
    "            return 1\n",
    "        if \"\".join(actual) == \"A\" and \"\".join(expected) == \"\":\n",
    "            return 1\n",
    "        \n",
    "        return score\n",
    "    else:\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text, task):\n",
    "    \"\"\"Normalize text by removing HTML tags and converting entities.\"\"\"\n",
    "    if task in ['panel_sequence', 'panel_source_assignment']:\n",
    "        return text\n",
    "    else:\n",
    "        if text is None:\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to string if needed\n",
    "        text = str(text)\n",
    "        \n",
    "        # Convert HTML entities to their corresponding characters\n",
    "        text = html.unescape(text)\n",
    "        \n",
    "        # Remove HTML tags using BeautifulSoup\n",
    "        soup = BeautifulSoup(text, 'html.parser')\n",
    "        text = soup.get_text()\n",
    "        \n",
    "        # Standardize whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu_score(reference, candidate):\n",
    "    \"\"\"Calculate BLEU score between reference and candidate texts.\"\"\"\n",
    "    if not reference or not candidate:\n",
    "        return 0.0\n",
    "    \n",
    "    # Tokenize texts\n",
    "    reference_tokens = nltk.word_tokenize(reference.lower())\n",
    "    candidate_tokens = nltk.word_tokenize(candidate.lower())\n",
    "    \n",
    "    # Use smoothing to handle zero counts\n",
    "    smoothie = SmoothingFunction().method1\n",
    "    \n",
    "    # Calculate BLEU score\n",
    "    try:\n",
    "        bleu_score = sentence_bleu([reference_tokens], candidate_tokens, \n",
    "                                 weights=(0.25, 0.25, 0.25, 0.25), \n",
    "                                 smoothing_function=smoothie)\n",
    "        return bleu_score\n",
    "    except Exception:\n",
    "        return 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recalculate_text_score(row):\n",
    "    # Text-based tasks\n",
    "    text_tasks = ['locate_figure_captions', 'extract_data_availability', \n",
    "                  'figure_title', 'figure_caption']\n",
    "    \n",
    "    # Only recalculate for text-based tasks\n",
    "    if row['task'] in text_tasks:\n",
    "        \n",
    "        # Calculate BLEU score\n",
    "        bleu_score = calculate_bleu_score(row['expected'], row['actual'])\n",
    "        \n",
    "        # Return BLEU score\n",
    "        return bleu_score\n",
    "    \n",
    "    # Return existing score for non-text tasks\n",
    "    return row['score']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot enhanced charts for each task\n",
    "def plot_enhanced_task_chart(df, task_name):\n",
    "    # Filter data for the specific task\n",
    "    task_data = df[df[\"task\"] == task_name].copy()\n",
    "    \n",
    "    # Create parameter group column\n",
    "    task_data['param_group'] = task_data.apply(\n",
    "        lambda row: f\"{row['model']}\\ntemp={row['temperature']}\\ntop_p={row['top_p']}\",\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Create figure with appropriate size\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Create boxplot\n",
    "    ax = sns.boxplot(data=task_data, x='param_group', y='score', color='lightblue')\n",
    "    \n",
    "    # Add individual data points using swarmplot\n",
    "    sns.swarmplot(data=task_data, x='param_group', y='score', color='black', alpha=0.5, size=4)\n",
    "    \n",
    "    # Calculate and annotate mean and median for each group\n",
    "    for i, group in enumerate(task_data['param_group'].unique()):\n",
    "        group_data = task_data[task_data['param_group'] == group]['score']\n",
    "        mean_score = group_data.mean()\n",
    "        median_score = group_data.median()\n",
    "        \n",
    "        # Annotate mean with red line\n",
    "        plt.hlines(y=mean_score, xmin=i-0.4, xmax=i+0.4, colors='red', linestyles='dashed', linewidth=2)\n",
    "        plt.text(i, mean_score + 0.01, f'Mean: {mean_score:.2f}', ha='center', va='bottom', color='red')\n",
    "        \n",
    "        # Annotate median with green marker\n",
    "        plt.text(i, median_score - 0.03, f'Median: {median_score:.2f}', ha='center', va='top', color='green')\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.title(f'Score Distribution for {task_name}', fontsize=15)\n",
    "    plt.xlabel('Model Parameters', fontsize=12)\n",
    "    plt.ylabel('Score', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Show plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_models_by_msid(df, task_name, models=None, temperatures=None):\n",
    "    \"\"\"\n",
    "    Plot multiple models' results by paper MSID.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame containing the benchmark data\n",
    "    - task_name: Task to analyze\n",
    "    - models: List of models to include (default: all models in the data)\n",
    "    - temperatures: List of temperatures to include (default: all temperatures)\n",
    "    \"\"\"\n",
    "    # Set default values if none provided\n",
    "    if models is None:\n",
    "        models = df['model'].unique()\n",
    "    if temperatures is None:\n",
    "        temperatures = df['temperature'].unique()\n",
    "    \n",
    "    # Filter data for the specified task, models, and temperatures\n",
    "    filtered_data = df[(df[\"task\"] == task_name) & \n",
    "                      (df[\"model\"].isin(models)) & \n",
    "                      (df[\"temperature\"].isin(temperatures))].copy()\n",
    "    \n",
    "    if len(filtered_data) == 0:\n",
    "        print(f\"No data found for task: {task_name} with the specified models and temperatures\")\n",
    "        return\n",
    "    \n",
    "    # Create figure with appropriate size\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    \n",
    "    # Create a model+temperature column for better visualization\n",
    "    filtered_data['model_temp'] = filtered_data.apply(\n",
    "        lambda row: f\"{row['model']} (t={row['temperature']})\", axis=1\n",
    "    )\n",
    "    \n",
    "    # Create boxplot grouped by msid, with different colors for each model\n",
    "    ax = sns.boxplot(\n",
    "        data=filtered_data, \n",
    "        x='msid', \n",
    "        y='score', \n",
    "        hue='model_temp',\n",
    "        palette='Set2'\n",
    "    )\n",
    "    \n",
    "    # Add individual data points (swarmplot would be too cluttered, using stripplot instead)\n",
    "    sns.stripplot(\n",
    "        data=filtered_data, \n",
    "        x='msid', \n",
    "        y='score', \n",
    "        hue='model_temp',\n",
    "        dodge=True,\n",
    "        alpha=0.5, \n",
    "        size=4,\n",
    "        palette='dark:black'\n",
    "    )\n",
    "    \n",
    "    # Calculate and annotate overall mean for each paper\n",
    "    msids = filtered_data['msid'].unique()\n",
    "    for i, msid in enumerate(msids):\n",
    "        paper_data = filtered_data[filtered_data['msid'] == msid]['score']\n",
    "        mean_score = paper_data.mean()\n",
    "        \n",
    "        # Annotate overall mean with black line\n",
    "        plt.hlines(y=mean_score, xmin=i-0.5, xmax=i+0.5, \n",
    "                  colors='black', linestyles='dashed', linewidth=2)\n",
    "        plt.text(i, mean_score + 0.02, f'Mean: {mean_score:.2f}', \n",
    "                ha='center', va='bottom', color='black', fontweight='bold')\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.title(f'Model Comparison by Paper MSID for {task_name}', fontsize=15)\n",
    "    plt.xlabel('Paper MSID', fontsize=12)\n",
    "    plt.ylabel('Score', fontsize=12)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add count of samples per paper\n",
    "    for i, msid in enumerate(msids):\n",
    "        count = len(filtered_data[filtered_data['msid'] == msid])\n",
    "        plt.text(i, -0.05, f'n={count}', ha='center', va='top', color='blue')\n",
    "    \n",
    "    # Improve legend position and size\n",
    "    plt.legend(title='Model (temperature)', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # Show plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics for reference\n",
    "    print(\"\\nSummary Statistics by Model for\", task_name)\n",
    "    summary = filtered_data.groupby('model_temp')['score'].agg(['mean', 'median', 'std', 'count']).round(3)\n",
    "    display(summary)\n",
    "\n",
    "\n",
    "# Define tasks\n",
    "tasks = ['locate_figure_captions', 'extract_data_availability', 'figure_title', \n",
    "         'figure_caption', 'panel_sequence', 'panel_source_assignment']\n",
    "\n",
    "# Example usage - plot all models for each task\n",
    "for task in tasks:\n",
    "    plot_models_by_msid(df, task)\n",
    "\n",
    "# Or you can specify which models and temperatures to include\n",
    "# models_to_compare = ['gpt-4o', 'claude-3-opus']\n",
    "# temperatures_to_compare = [0.0, 0.5]\n",
    "# plot_models_by_msid(df, 'figure_caption', models=models_to_compare, temperatures=temperatures_to_compare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(metrics_csv_path)\n",
    "\n",
    "df = df[~df.msid.isin([\"EMBOR-2024-59101V1-T\"])]\n",
    "\n",
    "df[\"actual\"] = df.apply(lambda row: normalize_text(row[\"actual\"], row[\"task\"]), axis=1)\n",
    "df[\"expected\"] = df.apply(lambda row: normalize_text(row[\"expected\"], row[\"task\"]), axis=1)\n",
    "\n",
    "df[\"score\"] = df.apply(\n",
    "    lambda row: score_panel_seq(\n",
    "        row[\"task\"],row[\"actual\"],row[\"expected\"],row[\"score\"]\n",
    "        ),\n",
    "    axis=1\n",
    "    )\n",
    "\n",
    "# Apply the score recalculation\n",
    "df[\"score\"] = df.apply(\n",
    "    lambda row: recalculate_text_score(row),\n",
    "    axis=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Benchmark results per model and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for each task\n",
    "tasks = ['locate_figure_captions', 'extract_data_availability', 'figure_title', \n",
    "         'figure_caption', 'panel_sequence', 'panel_source_assignment']\n",
    "\n",
    "for task in tasks:\n",
    "    plot_enhanced_task_chart(df, task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Benchmark results per paper, model, and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_compare = ['gpt-4o', 'gpt-4o-mini']\n",
    "temperatures_to_compare = [0.5, 1.0]\n",
    "for task in tasks:\n",
    "    plot_models_by_msid(df, 'task', models=models_to_compare, temperatures=temperatures_to_compare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Checking explicit errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_html_text_diff(expected, actual):\n",
    "    \"\"\"\n",
    "    Create HTML with color-coded text differences.\n",
    "    - Regular text: identical text\n",
    "    - Red text: text in expected but not in actual (deletions)\n",
    "    - Green text: text in actual but not in expected (insertions)\n",
    "    \"\"\"\n",
    "    # Convert to string if needed\n",
    "    expected = str(expected) if expected is not None else \"\"\n",
    "    actual = str(actual) if actual is not None else \"\"\n",
    "    \n",
    "    # Get diff operations\n",
    "    diff = difflib.SequenceMatcher(None, expected, actual)\n",
    "    \n",
    "    html = ['<div style=\"font-family: monospace; white-space: pre-wrap;\">']\n",
    "    \n",
    "    for op, i1, i2, j1, j2 in diff.get_opcodes():\n",
    "        if op == 'equal':\n",
    "            html.append(f'<span>{expected[i1:i2]}</span>')\n",
    "        elif op == 'delete':\n",
    "            html.append(f'<span style=\"color: red;\">{expected[i1:i2]}</span>')\n",
    "        elif op == 'insert':\n",
    "            html.append(f'<span style=\"color: green;\">{actual[j1:j2]}</span>')\n",
    "        elif op == 'replace':\n",
    "            html.append(f'<span style=\"color: red;\">{expected[i1:i2]}</span>')\n",
    "            html.append(f'<span style=\"color: green;\">{actual[j1:j2]}</span>')\n",
    "    \n",
    "    html.append('</div>')\n",
    "    return ''.join(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_html_list_diff(expected_list, actual_list):\n",
    "    \"\"\"\n",
    "    Create HTML with color-coded list differences.\n",
    "    - Green text: elements in both lists\n",
    "    - Red text: elements in only one list\n",
    "    \"\"\"\n",
    "    html = ['<div style=\"font-family: monospace;\">']\n",
    "    \n",
    "    html.append('<div><strong>Expected list:</strong></div><div>')\n",
    "    for item in expected_list:\n",
    "        if item in actual_list:\n",
    "            html.append(f'<span style=\"color: green; margin-right: 5px; padding: 2px;\">{item}</span>')\n",
    "        else:\n",
    "            html.append(f'<span style=\"color: red; margin-right: 5px; padding: 2px;\">{item}</span>')\n",
    "    html.append('</div>')\n",
    "    \n",
    "    html.append('<div><strong>Actual list:</strong></div><div>')\n",
    "    for item in actual_list:\n",
    "        if item in expected_list:\n",
    "            html.append(f'<span style=\"color: green; margin-right: 5px; padding: 2px;\">{item}</span>')\n",
    "        else:\n",
    "            html.append(f'<span style=\"color: red; margin-right: 5px; padding: 2px;\">{item}</span>')\n",
    "    html.append('</div>')\n",
    "    \n",
    "    html.append('</div>')\n",
    "    return ''.join(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_errors(df, task, score_threshold=0.8, n=5, seed=None):\n",
    "    \"\"\"\n",
    "    Visualize errors for a specific task.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame containing the benchmark data\n",
    "    - task: Task name to analyze\n",
    "    - score_threshold: Maximum score to consider as error (default: 0.8)\n",
    "    - n: Number of samples to display (default: 5)\n",
    "    - seed: Random seed for reproducibility (default: None)\n",
    "    \"\"\"\n",
    "    # Set random seed if provided\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    \n",
    "    # Filter data for the specific task\n",
    "    task_data = df[df[\"task\"] == task].copy()\n",
    "    \n",
    "    # Filter for errors (below score threshold)\n",
    "    errors = task_data[task_data[\"score\"] < score_threshold]\n",
    "    \n",
    "    if len(errors) == 0:\n",
    "        print(f\"No errors found for task '{task}' with score threshold {score_threshold}\")\n",
    "        return\n",
    "    \n",
    "    # Sample n random errors (or less if fewer are available)\n",
    "    n = min(n, len(errors))\n",
    "    error_samples = errors.sample(n)\n",
    "    \n",
    "    print(f\"Displaying {n} error samples for task '{task}' (score < {score_threshold})\")\n",
    "    print(f\"Total errors: {len(errors)} out of {len(task_data)} samples\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Text-based tasks\n",
    "    text_tasks = ['locate_figure_captions', 'extract_data_availability', 'figure_title', 'figure_caption']\n",
    "    # List-based tasks\n",
    "    list_tasks = ['panel_sequence', 'panel_source_assignment']\n",
    "    \n",
    "    # Display errors based on task type\n",
    "    if task in text_tasks:\n",
    "        for i, (_, row) in enumerate(error_samples.iterrows()):\n",
    "            print(f\"Sample {i+1} (Paper: {row['msid']}, Score: {row['score']:.2f})\")\n",
    "            \n",
    "            # Display actual and expected text\n",
    "            print(\"\\Input text:\")\n",
    "            print(row['input'])\n",
    "            print(\"\\nActual text:\")\n",
    "            print(row['actual'])\n",
    "            print(\"\\nExpected text:\")\n",
    "            print(row['expected'])\n",
    "            \n",
    "            # Display color-coded difference\n",
    "            print(\"\\nDifference:\")\n",
    "            html_diff = create_html_text_diff(row['expected'], row['actual'])\n",
    "            display(HTML(html_diff))\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "    elif task in list_tasks:\n",
    "        for i, (_, row) in enumerate(error_samples.iterrows()):\n",
    "            print(f\"Sample {i+1} (Paper: {row['msid']}, Score: {row['score']:.2f})\")\n",
    "            \n",
    "            # Convert string representations of lists to actual lists\n",
    "            try:\n",
    "                # Try to safely evaluate strings that should represent lists\n",
    "                # For panel_sequence, the format is usually something like \"['A', 'B', 'C']\"\n",
    "                # For panel_source_assignment, it might be more complex\n",
    "                actual_list = eval(row['actual']) if isinstance(row['actual'], str) else row['actual']\n",
    "                expected_list = eval(row['expected']) if isinstance(row['expected'], str) else row['expected']\n",
    "                \n",
    "                # Display color-coded lists\n",
    "                html_diff = create_html_list_diff(expected_list, actual_list)\n",
    "                display(HTML(html_diff))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing lists: {e}\")\n",
    "                print(\"Input: \", row['input'])\n",
    "                print(\"Raw actual:\", row['actual'])\n",
    "                print(\"Raw expected:\", row['expected'])\n",
    "            \n",
    "            print(\"-\" * 80)\n",
    "    else:\n",
    "        print(f\"Task '{task}' not recognized or not supported\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
