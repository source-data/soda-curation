{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "results_file = \"data/eval/results.json\"\n",
    "score = \"bleu1\"\n",
    "ignore = {\n",
    "    \"task\": None,\n",
    "    \"strategy\": [\"regex\"],\n",
    "    \"msid\": None,\n",
    "    \"figure_label\": None,\n",
    "    \"run\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(results_file)\n",
    "\n",
    "# remove runs in ignore\n",
    "for k, v in ignore.items():\n",
    "    if v is not None:\n",
    "        df = df[~df[k].isin(v)]\n",
    "\n",
    "# run is a number, but should be considered a category\n",
    "df = df.assign(run=df.run.astype('category'))\n",
    "\n",
    "n_tasks = df.task.nunique()\n",
    "n_strategies = df.strategy.nunique()\n",
    "n_msids = df.msid.nunique()\n",
    "n_figures = df.figure_label.nunique()\n",
    "n_runs = df.run.nunique()\n",
    "\n",
    "assert df[score].min() >= 0, \"Score should be non-negative\"\n",
    "assert df[score].max() <= 1, \"Score should be at most 1\"\n",
    "\n",
    "# split into tasks\n",
    "legends = df[df[\"task\"] == \"extract_figure_legends\"]\n",
    "labels = df[df[\"task\"] == \"extract_figures\"]\n",
    "titles = df[df[\"task\"] == \"extract_figure_title\"]\n",
    "captions = df[df[\"task\"] == \"extract_figure_caption\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-line stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"task\", \"strategy\"])[[score]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def add_threshold(df, fig, axis=\"y\"):\n",
    "    score_threshold = df[f\"{score}_threshold\"].iloc[0].round(2)\n",
    "    kwargs = dict(\n",
    "        line_dash=\"dash\",\n",
    "        line_color=\"green\",\n",
    "        opacity=0.5,\n",
    "        annotation_text=f\"threshold: {score_threshold}\",\n",
    "        annotation_position=\"top left\",\n",
    "    )\n",
    "    if axis == \"y\":\n",
    "        fig.add_hline(y=score_threshold, **kwargs)\n",
    "    else:\n",
    "        fig.add_vline(x=score_threshold, **kwargs)\n",
    "\n",
    "def hist_score(df, interval_width=0.01, **kwargs):\n",
    "    groupby = [\"task\", \"strategy\"]\n",
    "    x_start = 0 - interval_width\n",
    "    x_end = 1 + interval_width\n",
    "    y_start = -1\n",
    "    y_end = df.groupby(groupby).count().max().max() + 1\n",
    "    fig = px.histogram(\n",
    "        df,\n",
    "        x=score,\n",
    "        range_x=[x_start, x_end],\n",
    "        range_y=[y_start, y_end],\n",
    "        facet_col=\"task\",\n",
    "        facet_row=\"strategy\",\n",
    "        width=900,\n",
    "        height=100 + 250 * n_strategies,\n",
    "        **kwargs,\n",
    "    )\n",
    "    fig.update_traces(\n",
    "        xbins=dict( # bins used for histogram\n",
    "            start=x_start,\n",
    "            end=x_end,\n",
    "            size=interval_width,\n",
    "        )\n",
    "    )\n",
    "    fig.update_layout(bargap=0.2)\n",
    "    add_threshold(df, fig, axis=\"x\")\n",
    "    return fig\n",
    "\n",
    "def scatter_score(df, **kwargs):\n",
    "    fig = px.scatter(\n",
    "        df,\n",
    "        x=\"msid\",\n",
    "        y=score,\n",
    "        facet_col=\"task\",\n",
    "        facet_row=\"strategy\",\n",
    "        color=\"run\",\n",
    "        height=100 + 250 * n_strategies,\n",
    "        width=900,\n",
    "        **kwargs,\n",
    "    )\n",
    "    fig.update_traces(marker_size=10)\n",
    "    fig.update_layout(scattermode=\"group\", scattergap=0.9)\n",
    "    add_threshold(df, fig)\n",
    "    return fig\n",
    "\n",
    "def plot_std(df, groupby, **kwargs):\n",
    "    df_std = (\n",
    "        df.groupby(groupby)\n",
    "        [score].std()\n",
    "        .reset_index()\n",
    "        .sort_values(groupby)\n",
    "        .rename(columns={score: f\"{score}_std\"})\n",
    "    )\n",
    "    fig = px.scatter(\n",
    "        df_std,\n",
    "        x=\"msid\",\n",
    "        y=f\"{score}_std\",\n",
    "        facet_col=\"task\",\n",
    "        facet_row=\"strategy\",\n",
    "        height=100 + 250 * n_strategies,\n",
    "        width=900,\n",
    "        **kwargs,\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_df in [legends, labels]:\n",
    "    if task_df.empty:\n",
    "        continue\n",
    "    hist_score(task_df).show()\n",
    "    scatter_score(task_df).show()\n",
    "    plot_std(task_df, groupby=[\"task\", \"strategy\", \"msid\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_df in [titles, captions]:\n",
    "    if task_df.empty:\n",
    "        continue\n",
    "    hist_score(task_df).show()\n",
    "    scatter_score(\n",
    "        task_df,\n",
    "        symbol=\"figure_label\",\n",
    "    ).show()\n",
    "    plot_std(\n",
    "        task_df,\n",
    "        groupby=[\"task\", \"strategy\", \"msid\", \"figure_label\"],\n",
    "        color=\"figure_label\",\n",
    "        symbol=\"figure_label\",\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "import html\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def inline_diff(a, b):\n",
    "    matcher = difflib.SequenceMatcher(None, a, b)\n",
    "    def process_tag(tag, i1, i2, j1, j2):\n",
    "        a_text = html.escape(a[i1:i2])\n",
    "        b_text = html.escape(b[j1:j2])\n",
    "        if tag == 'delete':\n",
    "            return '<del>' + a_text + '</del>'\n",
    "        if tag == 'equal':\n",
    "            return a_text\n",
    "        if tag == 'insert':\n",
    "            return '<ins>' + b_text + '</ins>'\n",
    "        if tag == 'replace':\n",
    "            # combine as delete + insert\n",
    "            return '<del>' + a_text + '</del><ins>' + b_text + '</ins>'\n",
    "        assert False, \"Unknown tag %r\"%tag\n",
    "    return ''.join(process_tag(*t) for t in matcher.get_opcodes())\n",
    "\n",
    "def display_diff_(row):\n",
    "    diff_css = \"\"\"\n",
    "    <style>\n",
    "    ins {background-color: #588a41;}  // light green\n",
    "    del {background-color: #ffaaaa;}  // light red\n",
    "    repl {background-color: #bb99ff;} // light purple\n",
    "    table tr > * { width: 50%; }\n",
    "    table tr > td { vertical-align: top; text-align: left; }\n",
    "    </style>\n",
    "    \"\"\"\n",
    "\n",
    "    # run_name = (\n",
    "    #     f\"{row.task} - {row.strategy} - {row.msid} - {row.run}\"\n",
    "    #     if row.task == \"extract_figure_caption\"\n",
    "    #     else f\"{row.task} - {row.strategy} - {row.msid} - {row.figure_label} - {row.run}\"\n",
    "    # )\n",
    "    # header = f\"<h3>{run_name}</h3>\"\n",
    "    section_fn = lambda title, content: f\"<h4>{title}</h4><p>{content}</p>\"\n",
    "    # scores = section_fn(\"Scores\", f\"bleu1: {row.bleu1[0]:.2f}}\")\n",
    "\n",
    "    comparison = f\"\"\"\n",
    "        <table>\n",
    "            <thead>\n",
    "                <tr>\n",
    "                    <th>Expected</th>\n",
    "                    <th>Actual</th>\n",
    "                </tr>\n",
    "            </thead>\n",
    "            <tbody>\n",
    "                <tr>\n",
    "                    <td>{html.escape(row.head(1)[\"expected\"].values[0])}</td>\n",
    "                    <td>{html.escape(row.head(1)[\"actual\"].values[0])}</td>\n",
    "                </tr>\n",
    "            </tbody>\n",
    "        </table>\n",
    "    \"\"\"\n",
    "    diff = section_fn(\"Diff\", inline_diff(row.head(1)[\"expected\"].values[0], row.head(1)[\"actual\"].values[0]) + comparison)\n",
    "    input_text = section_fn(\"Input\", row.input)\n",
    "\n",
    "    # display(HTML(diff_css + header + scores + diff + input_text))\n",
    "    display(HTML(diff_css + diff + input_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['extract_figure_legends', None, 'extract_figures',\n",
       "       'extract_figure_title', 'extract_figure_caption'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"task\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bad_figure_legends = df[(df[\"task\"]==\"extract_figure_legends\") & (df[\"bleu1\"]<0.95)]\n",
    "df_bad_labels = df[(df[\"task\"]==\"extract_figures\") & (df[\"bleu1\"]<0.95)]\n",
    "df_bad_titles = df[(df[\"task\"]==\"extract_figure_title\") & (df[\"bleu1\"]<0.95)]\n",
    "df_bad_caption = df[(df[\"task\"]==\"extract_figure_caption\") & (df[\"bleu1\"]<0.95)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of bad detection of figure section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No bad figure legends\n"
     ]
    }
   ],
   "source": [
    "display_diff_(df_bad_figure_legends.sample()) if len(df_bad_figure_legends) > 0 else print(\"No bad figure legends\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of bad detection of figure label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    ins {background-color: #588a41;}  // light green\n",
       "    del {background-color: #ffaaaa;}  // light red\n",
       "    repl {background-color: #bb99ff;} // light purple\n",
       "    table tr > * { width: 50%; }\n",
       "    table tr > td { vertical-align: top; text-align: left; }\n",
       "    </style>\n",
       "    <h4>Diff</h4><p><del>Figure 1\n",
       "Figure 2\n",
       "Figure 3\n",
       "Figure 4\n",
       "Figure 5\n",
       "Figure 6\n",
       "</del>Figure 7\n",
       "Figure 8\n",
       "        <table>\n",
       "            <thead>\n",
       "                <tr>\n",
       "                    <th>Expected</th>\n",
       "                    <th>Actual</th>\n",
       "                </tr>\n",
       "            </thead>\n",
       "            <tbody>\n",
       "                <tr>\n",
       "                    <td>Figure 1\n",
       "Figure 2\n",
       "Figure 3\n",
       "Figure 4\n",
       "Figure 5\n",
       "Figure 6\n",
       "Figure 7\n",
       "Figure 8</td>\n",
       "                    <td>Figure 7\n",
       "Figure 8</td>\n",
       "                </tr>\n",
       "            </tbody>\n",
       "        </table>\n",
       "    </p><h4>Input</h4><p>694    <p><strong>Figure legends</strong></p>\\n<p><st...\n",
       "Name: input, dtype: object</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_diff_(df_bad_labels.sample()) if len(df_bad_labels) > 0 else print(\"No bad figure labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of bad figure titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    ins {background-color: #588a41;}  // light green\n",
       "    del {background-color: #ffaaaa;}  // light red\n",
       "    repl {background-color: #bb99ff;} // light purple\n",
       "    table tr > * { width: 50%; }\n",
       "    table tr > td { vertical-align: top; text-align: left; }\n",
       "    </style>\n",
       "    <h4>Diff</h4><p><del>&lt;strong&gt;</del>Recruitment of Nedd4L to membrane<del>\n",
       "</del><ins> </ins>tubules generated by FCHO2 in cells.<del>&lt;/strong&gt;</del>\n",
       "        <table>\n",
       "            <thead>\n",
       "                <tr>\n",
       "                    <th>Expected</th>\n",
       "                    <th>Actual</th>\n",
       "                </tr>\n",
       "            </thead>\n",
       "            <tbody>\n",
       "                <tr>\n",
       "                    <td>&lt;strong&gt;Recruitment of Nedd4L to membrane\n",
       "tubules generated by FCHO2 in cells.&lt;/strong&gt;</td>\n",
       "                    <td>Recruitment of Nedd4L to membrane tubules generated by FCHO2 in cells.</td>\n",
       "                </tr>\n",
       "            </tbody>\n",
       "        </table>\n",
       "    </p><h4>Input</h4><p>1489    <strong>Figure legends</strong></p>\\n<p><stron...\n",
       "Name: input, dtype: object</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_diff_(df_bad_titles.sample()) if len(df_bad_titles) > 0 else print(\"No bad figure titles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of bad figure captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    ins {background-color: #588a41;}  // light green\n",
       "    del {background-color: #ffaaaa;}  // light red\n",
       "    repl {background-color: #bb99ff;} // light purple\n",
       "    table tr > * { width: 50%; }\n",
       "    table tr > td { vertical-align: top; text-align: left; }\n",
       "    </style>\n",
       "    <h4>Diff</h4><p><del>(&lt;stron</del><ins>Fi</ins>g<del>&gt;A)&lt;/stron</del><ins>ure 2-The ERAD client NHK is re-directed to LAMP1-positive endolysosomes for de</ins>g<del>&gt;</del><ins>radation upon pharmacologic inactivation of ERAD. (A)</ins> HEK239 cells transiently<del>\n",
       "</del><ins> </ins>expressing the <del>&lt;sup&gt;</del>35<del>&lt;/sup&gt;</del>S-NHK are chased for the indicated time<del>\n",
       "</del><ins> </ins>without (lanes 1-3) or with PS341 (lanes 4, 5), PS341/BafA1 (lanes 6,<del>\n",
       "</del><ins> </ins>7), KIF (lanes 8, 9), or KIF/BafA1 (10-11). Radiolabeled NHK is<del>\n",
       "immunoisolated at the end of the indicated chase time and separated in\n",
       "</del><ins> immunoisolated at the end of the indicated chase time and separated in </ins>SDS-PAGE. Densitometric quantification of the <del>&lt;sup&gt;</del>35<del>&lt;/sup&gt;</del>S-NHK band in<del>\n",
       "</del><ins> </ins>this representative experiment is shown. (<del>&lt;strong&gt;</del>B)<del>&lt;/strong&gt;\n",
       "</del><ins> </ins>Quantification of <del>&lt;sup&gt;</del>35<del>&lt;/sup&gt;</del>S-NHK after a 10- or 120-min chase in<del>\n",
       "</del><ins> </ins>cells mock treated or exposed to PS341 or PS341/BafA1. Individual data<del>\n",
       "</del><ins> </ins>points at 120-min (%, normalized to 10’ mock): mock= 49, 37 and 49%;<del>\n",
       "</del><ins> </ins>PS341= 73, 55 and 52%; PS341/BafA1= 91, 63 and 68%. (<del>&lt;strong&gt;</del>C)<del>&lt;/strong&gt;\n",
       "</del><ins> </ins>Same as (<del>&lt;strong&gt;</del>B)<del>&lt;/strong&gt;</del> for Kif, KIF/BafA1. Individual data points<del>\n",
       "</del><ins> </ins>at 120-min (%, normalized to 10-min mock): mock= 49, 37 and 49%; KIF=<del>\n",
       "</del><ins> </ins>80, 53 and 54%; KIF/BafA1= 99, 64 and 83%. <del>&lt;strong&gt;</del>(D)<del>&lt;/strong&gt;</del> BafA1<del>\n",
       "</del><ins> </ins>does not inhibit clearance of the ERAD client NHK. n=3 for 10- and<del>\n",
       "</del><ins> </ins>120-min; n=2 for 120-min BafA1. (<del>&lt;strong&gt;</del>E)<del>&lt;/strong&gt;</del> ATZ, a canonical<del>\n",
       "</del><ins> </ins>ERLAD client accumulates within LAMP1-positive endolysosomes in 3T3<del>\n",
       "</del><ins> </ins>cells exposed to BafA1. (<del>&lt;strong&gt;</del>F)<del>&lt;/strong&gt;</del> The ERAD client NHK is not<del>\n",
       "</del><ins> </ins>delivered within LAMP1-positive endolysosomes. (<del>&lt;strong&gt;</del>G-H)<del>&lt;/strong&gt;\n",
       "</del><ins> </ins>NHK is delivered and accumulates within endolysosomes inactivated with<del>\n",
       "</del><ins> </ins>BafA1 upon ERAD inhibition with KIF (G) or with PS341 (H).<del>\n",
       "(&lt;strong&gt;</del><ins> (</ins>I)<del>&lt;/strong&gt;</del> LysoQuant quantification of ATZ delivery within<del>\n",
       "</del><ins> </ins>LAMP1-positive endolysosomes in panel <del>&lt;strong&gt;</del>E<del>&lt;/strong&gt;</del>, and of NHK in<del>\n",
       "panels &lt;strong&gt;</del><ins> panels </ins>F-H<del>&lt;/strong&gt;</del> (n=18, 19, 23 and 13 cells, respectively.<del>\n",
       "(&lt;strong&gt;</del><ins> (</ins>J-N)<del>&lt;/strong&gt;</del> Analysis of NHK delivery to LAMP1-positive<del>\n",
       "</del><ins> </ins>endolysosomes as in (F-H) but with drug incubation of 8h and with<del>\n",
       "</del><ins> </ins>additional BafA1 washout conditions (L, N).<del>&lt;strong&gt;</del><ins> </ins>O<del>&lt;/strong&gt;</del> LysoQuant<del>\n",
       "</del><ins> </ins>quantification of NHK accumulation within LAMP1 positive endolysosomes<del>\n",
       "(panels &lt;strong&gt;</del><ins> (panels </ins>J-N<del>&lt;/strong&gt;</del>, n=31, 24, 31, 31 and 33 cells<del>\n",
       "</del><ins> </ins>respectively.<del>&lt;/p&gt;\n",
       "&lt;p&gt;</del><ins> </ins>Data Information: (B, C) N=3 independent experiments, mean +/- SEM is<del>\n",
       "</del><ins> </ins>shown. (D) mean +/- SEM is shown.(I, O) mean is shown. N=3 independent<del>\n",
       "</del><ins> </ins>experiments. One-way analysis of variance (ANOVA) and Dunnett’s multiple<del>\n",
       "</del><ins> </ins>comparison test, ****P<del>&amp;lt;</del><ins>&lt;</ins>0.0001. (E-H, J-N) Scale bars: 10μM, Insets<del>\n",
       "</del><ins> </ins>are shown with white squares.<del>&lt;/p&gt;\n",
       "&lt;p&gt;&lt;strong&gt;&lt;br /&gt;\n",
       "&lt;/strong&gt;&lt;/p&gt;</del>\n",
       "        <table>\n",
       "            <thead>\n",
       "                <tr>\n",
       "                    <th>Expected</th>\n",
       "                    <th>Actual</th>\n",
       "                </tr>\n",
       "            </thead>\n",
       "            <tbody>\n",
       "                <tr>\n",
       "                    <td>(&lt;strong&gt;A)&lt;/strong&gt; HEK239 cells transiently\n",
       "expressing the &lt;sup&gt;35&lt;/sup&gt;S-NHK are chased for the indicated time\n",
       "without (lanes 1-3) or with PS341 (lanes 4, 5), PS341/BafA1 (lanes 6,\n",
       "7), KIF (lanes 8, 9), or KIF/BafA1 (10-11). Radiolabeled NHK is\n",
       "immunoisolated at the end of the indicated chase time and separated in\n",
       "SDS-PAGE. Densitometric quantification of the &lt;sup&gt;35&lt;/sup&gt;S-NHK band in\n",
       "this representative experiment is shown. (&lt;strong&gt;B)&lt;/strong&gt;\n",
       "Quantification of &lt;sup&gt;35&lt;/sup&gt;S-NHK after a 10- or 120-min chase in\n",
       "cells mock treated or exposed to PS341 or PS341/BafA1. Individual data\n",
       "points at 120-min (%, normalized to 10’ mock): mock= 49, 37 and 49%;\n",
       "PS341= 73, 55 and 52%; PS341/BafA1= 91, 63 and 68%. (&lt;strong&gt;C)&lt;/strong&gt;\n",
       "Same as (&lt;strong&gt;B)&lt;/strong&gt; for Kif, KIF/BafA1. Individual data points\n",
       "at 120-min (%, normalized to 10-min mock): mock= 49, 37 and 49%; KIF=\n",
       "80, 53 and 54%; KIF/BafA1= 99, 64 and 83%. &lt;strong&gt;(D)&lt;/strong&gt; BafA1\n",
       "does not inhibit clearance of the ERAD client NHK. n=3 for 10- and\n",
       "120-min; n=2 for 120-min BafA1. (&lt;strong&gt;E)&lt;/strong&gt; ATZ, a canonical\n",
       "ERLAD client accumulates within LAMP1-positive endolysosomes in 3T3\n",
       "cells exposed to BafA1. (&lt;strong&gt;F)&lt;/strong&gt; The ERAD client NHK is not\n",
       "delivered within LAMP1-positive endolysosomes. (&lt;strong&gt;G-H)&lt;/strong&gt;\n",
       "NHK is delivered and accumulates within endolysosomes inactivated with\n",
       "BafA1 upon ERAD inhibition with KIF (G) or with PS341 (H).\n",
       "(&lt;strong&gt;I)&lt;/strong&gt; LysoQuant quantification of ATZ delivery within\n",
       "LAMP1-positive endolysosomes in panel &lt;strong&gt;E&lt;/strong&gt;, and of NHK in\n",
       "panels &lt;strong&gt;F-H&lt;/strong&gt; (n=18, 19, 23 and 13 cells, respectively.\n",
       "(&lt;strong&gt;J-N)&lt;/strong&gt; Analysis of NHK delivery to LAMP1-positive\n",
       "endolysosomes as in (F-H) but with drug incubation of 8h and with\n",
       "additional BafA1 washout conditions (L, N).&lt;strong&gt;O&lt;/strong&gt; LysoQuant\n",
       "quantification of NHK accumulation within LAMP1 positive endolysosomes\n",
       "(panels &lt;strong&gt;J-N&lt;/strong&gt;, n=31, 24, 31, 31 and 33 cells\n",
       "respectively.&lt;/p&gt;\n",
       "&lt;p&gt;Data Information: (B, C) N=3 independent experiments, mean +/- SEM is\n",
       "shown. (D) mean +/- SEM is shown.(I, O) mean is shown. N=3 independent\n",
       "experiments. One-way analysis of variance (ANOVA) and Dunnett’s multiple\n",
       "comparison test, ****P&amp;lt;0.0001. (E-H, J-N) Scale bars: 10μM, Insets\n",
       "are shown with white squares.&lt;/p&gt;\n",
       "&lt;p&gt;&lt;strong&gt;&lt;br /&gt;\n",
       "&lt;/strong&gt;&lt;/p&gt;</td>\n",
       "                    <td>Figure 2-The ERAD client NHK is re-directed to LAMP1-positive endolysosomes for degradation upon pharmacologic inactivation of ERAD. (A) HEK239 cells transiently expressing the 35S-NHK are chased for the indicated time without (lanes 1-3) or with PS341 (lanes 4, 5), PS341/BafA1 (lanes 6, 7), KIF (lanes 8, 9), or KIF/BafA1 (10-11). Radiolabeled NHK is immunoisolated at the end of the indicated chase time and separated in SDS-PAGE. Densitometric quantification of the 35S-NHK band in this representative experiment is shown. (B) Quantification of 35S-NHK after a 10- or 120-min chase in cells mock treated or exposed to PS341 or PS341/BafA1. Individual data points at 120-min (%, normalized to 10’ mock): mock= 49, 37 and 49%; PS341= 73, 55 and 52%; PS341/BafA1= 91, 63 and 68%. (C) Same as (B) for Kif, KIF/BafA1. Individual data points at 120-min (%, normalized to 10-min mock): mock= 49, 37 and 49%; KIF= 80, 53 and 54%; KIF/BafA1= 99, 64 and 83%. (D) BafA1 does not inhibit clearance of the ERAD client NHK. n=3 for 10- and 120-min; n=2 for 120-min BafA1. (E) ATZ, a canonical ERLAD client accumulates within LAMP1-positive endolysosomes in 3T3 cells exposed to BafA1. (F) The ERAD client NHK is not delivered within LAMP1-positive endolysosomes. (G-H) NHK is delivered and accumulates within endolysosomes inactivated with BafA1 upon ERAD inhibition with KIF (G) or with PS341 (H). (I) LysoQuant quantification of ATZ delivery within LAMP1-positive endolysosomes in panel E, and of NHK in panels F-H (n=18, 19, 23 and 13 cells, respectively. (J-N) Analysis of NHK delivery to LAMP1-positive endolysosomes as in (F-H) but with drug incubation of 8h and with additional BafA1 washout conditions (L, N). O LysoQuant quantification of NHK accumulation within LAMP1 positive endolysosomes (panels J-N, n=31, 24, 31, 31 and 33 cells respectively. Data Information: (B, C) N=3 independent experiments, mean +/- SEM is shown. (D) mean +/- SEM is shown.(I, O) mean is shown. N=3 independent experiments. One-way analysis of variance (ANOVA) and Dunnett’s multiple comparison test, ****P&lt;0.0001. (E-H, J-N) Scale bars: 10μM, Insets are shown with white squares.</td>\n",
       "                </tr>\n",
       "            </tbody>\n",
       "        </table>\n",
       "    </p><h4>Input</h4><p>5095    </p>\\n<p><strong>Figure 1</strong>-Proteasomal...\n",
       "Name: input, dtype: object</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_diff_(df_bad_caption.sample()) if len(df_bad_caption) > 0 else print(\"No bad figure captions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import difflib\n",
    "# import html\n",
    "# from IPython.display import display, HTML\n",
    "\n",
    "# def inline_diff(a, b):\n",
    "#     matcher = difflib.SequenceMatcher(None, a, b)\n",
    "#     def process_tag(tag, i1, i2, j1, j2):\n",
    "#         a_text = html.escape(a[i1:i2])\n",
    "#         b_text = html.escape(b[j1:j2])\n",
    "#         if tag == 'delete':\n",
    "#             return '<del>' + a_text + '</del>'\n",
    "#         if tag == 'equal':\n",
    "#             return a_text\n",
    "#         if tag == 'insert':\n",
    "#             return '<ins>' + b_text + '</ins>'\n",
    "#         if tag == 'replace':\n",
    "#             # combine as delete + insert\n",
    "#             return '<del>' + a_text + '</del><ins>' + b_text + '</ins>'\n",
    "#         assert False, \"Unknown tag %r\"%tag\n",
    "#     return ''.join(process_tag(*t) for t in matcher.get_opcodes())\n",
    "\n",
    "# def display_diff(row):\n",
    "#     diff_css = \"\"\"\n",
    "#     <style>\n",
    "#     ins {background-color: #aaffaa;}  // light green\n",
    "#     del {background-color: #ffaaaa;}  // light red\n",
    "#     repl {background-color: #bb99ff;} // light purple\n",
    "#     table tr > * { width: 50%; }\n",
    "#     table tr > td { vertical-align: top; text-align: left; }\n",
    "#     </style>\n",
    "#     \"\"\"\n",
    "\n",
    "#     run_name = (\n",
    "#         f\"{row.task} - {row.strategy} - {row.msid} - {row.run}\"\n",
    "#         if row.task == \"extract_figure_legends\"\n",
    "#         else f\"{row.task} - {row.strategy} - {row.msid} - {row.figure_label} - {row.run}\"\n",
    "#     )\n",
    "#     header = f\"<h3>{run_name}</h3>\"\n",
    "#     section_fn = lambda title, content: f\"<h4>{title}</h4><p>{content}</p>\"\n",
    "#     scores = section_fn(\"Scores\", f\"bleu1: {row.bleu1:.2f}, bleu2: {row.bleu2:.2f}, bleu3: {row.bleu3:.2f}, bleu4: {row.bleu4:.2f}, rouge1: {row.rouge1:.2f}, rouge2: {row.rouge2:.2f}, rougeL: {row.rougeL:.2f}\")\n",
    "\n",
    "#     comparison = f\"\"\"\n",
    "# <table>\n",
    "#     <thead>\n",
    "#         <tr>\n",
    "#             <th>Expected</th>\n",
    "#             <th>Actual</th>\n",
    "#         </tr>\n",
    "#     </thead>\n",
    "#     <tbody>\n",
    "#         <tr>\n",
    "#             <td>{html.escape(row.expected)}</td>\n",
    "#             <td>{html.escape(row.actual)}</td>\n",
    "#         </tr>\n",
    "#     </tbody>\n",
    "# </table>\"\"\"\n",
    "#     diff = section_fn(\"Diff\", inline_diff(row.expected, row.actual) + comparison)\n",
    "#     input_text = section_fn(\"Input\", row.input)\n",
    "\n",
    "#     display(HTML(diff_css + header + scores + diff + input_text))\n",
    "\n",
    "# def display_all(df):\n",
    "#     print(f\"Showing {len(df)} runs\")\n",
    "#     for row in df.itertuples():\n",
    "#         display_diff(row)\n",
    "\n",
    "# def show_runs(\n",
    "#     df,\n",
    "#     tasks=None,\n",
    "#     strategies=None,\n",
    "#     msids=None,\n",
    "#     figure_labels=None,\n",
    "#     runs=None,\n",
    "#     score_range=None,\n",
    "# ):\n",
    "#     if tasks is not None:\n",
    "#         df = df[df.task.isin(tasks)]\n",
    "#     if strategies is not None:\n",
    "#         df = df[df.strategy.isin(strategies)]\n",
    "#     if msids is not None:\n",
    "#         df = df[df.msid.isin(msids)]\n",
    "#     if figure_labels is not None:\n",
    "#         df = df[df.figure_label.isin(figure_labels)]\n",
    "#     if runs is not None:\n",
    "#         df = df[df.run.isin(runs)]\n",
    "#     if score_range is not None:\n",
    "#         min_score, max_score = score_range\n",
    "#         df = df[(df[score] >= min_score) & (df[score] < max_score)]\n",
    "#     return display_all(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for task_df in [legends, labels, titles, captions]:\n",
    "#     if task_df.empty:\n",
    "#         continue\n",
    "#     print(f\"Task: {task_df.task.iloc[0]}\")\n",
    "#     show_runs(task_df, score_range=(0., 0.949))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soda-curation-H-EFKjGu-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
