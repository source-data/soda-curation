{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_file = \"data/eval/results.json\"\n",
    "score = \"bleu1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(results_file)\n",
    "\n",
    "# only look at the latest round\n",
    "latest_round = df.timestamp.max()\n",
    "df = df[df.timestamp == latest_round]\n",
    "\n",
    "# run is a number, but should be considered a category\n",
    "df = df.assign(run=df.run.astype('category'))\n",
    "\n",
    "n_tasks = df.task.nunique()\n",
    "n_strategies = df.strategy.nunique()\n",
    "n_msids = df.msid.nunique()\n",
    "n_figures = df.figure_label.nunique()\n",
    "n_runs = df.run.nunique()\n",
    "\n",
    "assert df[score].min() >= 0, \"Score should be non-negative\"\n",
    "assert df[score].max() <= 1, \"Score should be at most 1\"\n",
    "\n",
    "# split into tasks\n",
    "legends = df[df[\"task\"] == \"extract_figure_legends\"]\n",
    "captions = df[df[\"task\"] == \"extract_figure_caption\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def add_threshold(df, fig, axis=\"y\"):\n",
    "    score_threshold = df[f\"{score}_threshold\"].iloc[0].round(2)\n",
    "    kwargs = dict(\n",
    "        line_dash=\"dash\",\n",
    "        line_color=\"green\",\n",
    "        opacity=0.5,\n",
    "        annotation_text=f\"threshold: {score_threshold}\",\n",
    "        annotation_position=\"top left\",\n",
    "    )\n",
    "    if axis == \"y\":\n",
    "        fig.add_hline(y=score_threshold, **kwargs)\n",
    "    else:\n",
    "        fig.add_vline(x=score_threshold, **kwargs)\n",
    "\n",
    "def violin(df, **kwargs):\n",
    "    fig = px.violin(\n",
    "        df,\n",
    "        x=\"strategy\",\n",
    "        y=score,\n",
    "        box=True,\n",
    "        # points=\"all\",\n",
    "        **kwargs,\n",
    "    )\n",
    "    add_threshold(df, fig)\n",
    "    return fig\n",
    "\n",
    "def box(df, **kwargs):\n",
    "    fig = px.box(\n",
    "        df,\n",
    "        x=\"strategy\",\n",
    "        y=score,\n",
    "        points=\"all\",\n",
    "        **kwargs,\n",
    "    )\n",
    "    add_threshold(df, fig)\n",
    "    return fig\n",
    "\n",
    "def histogram(df, interval_width=0.01, **kwargs):\n",
    "    fig = px.histogram(\n",
    "        df,\n",
    "        x=score,\n",
    "        nbins=int(1 / interval_width),\n",
    "        **kwargs,\n",
    "    )\n",
    "    fig.update_layout(bargap=0.2)\n",
    "    add_threshold(df, fig, axis=\"x\")\n",
    "    return fig\n",
    "\n",
    "def scatter(df, **kwargs):\n",
    "    fig = px.scatter(\n",
    "        df,\n",
    "        x=\"strategy\",\n",
    "        y=score,\n",
    "        **kwargs,\n",
    "    )\n",
    "    fig.update_traces(marker_size=10)\n",
    "    fig.update_layout(scattermode=\"group\", scattergap=0.9)\n",
    "    add_threshold(df, fig)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_std(df, groupby):\n",
    "    return df.groupby(groupby)[score].std().reset_index()\n",
    "\n",
    "def plot_std(df, groupby, **kwargs):\n",
    "    df_std = get_std(df, groupby).sort_values(groupby).rename(columns={score: f\"{score}_std\"})\n",
    "    fig = px.scatter(\n",
    "        df_std,\n",
    "        x=f\"{score}_std\",\n",
    "        marginal_x=\"histogram\",\n",
    "        **kwargs,\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-line stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"task\", \"strategy\"])[[score]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the figure legends section from manuscripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram(legends, facet_col=\"strategy\", facet_col_wrap=2, width=2 * 600, title=\"Extracting the figure legends section from the full manuscript text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(legends, facet_col=\"msid\", facet_col_wrap=4, color=\"run\", title=\"Extracting the figure legends section from the full manuscript text\", height=int(1 + n_msids / 4) * 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_std(\n",
    "    legends,\n",
    "    [\"strategy\", \"msid\"],\n",
    "    y=\"msid\",\n",
    "    facet_col=\"strategy\",\n",
    "    width=400 + 300 * n_strategies,\n",
    "    title=f\"Standard deviation of {score} scores between runs of extracting figure legends from the same manuscript text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting individual figure captions from figure legends sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram(captions, facet_col=\"strategy\", title=\"Extracting individual figure captions from figure legends sections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(captions, facet_col=\"msid\", facet_col_wrap=3, symbol=\"figure_label\", color=\"run\", title=\"Extracting individual figure captions from figure legends sections\", height=1600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_std(\n",
    "    captions,\n",
    "    [\"strategy\", \"msid\", \"figure_label\"],\n",
    "    y=\"msid\",\n",
    "    facet_col=\"strategy\",\n",
    "    facet_col_spacing=0.1,\n",
    "    symbol=\"figure_label\",\n",
    "    height=200 + 50 * n_msids,\n",
    "    width=400 + 400 * n_strategies,\n",
    "    title=f\"Standard deviation of {score} scores between runs of extracting individual figure captions from the same figure legends section\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "import html\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def inline_diff(a, b):\n",
    "    matcher = difflib.SequenceMatcher(None, a, b)\n",
    "    def process_tag(tag, i1, i2, j1, j2):\n",
    "        a_text = html.escape(a[i1:i2])\n",
    "        b_text = html.escape(b[j1:j2])\n",
    "        if tag == 'delete':\n",
    "            return '<del>' + a_text + '</del>'\n",
    "        if tag == 'equal':\n",
    "            return a_text\n",
    "        if tag == 'insert':\n",
    "            return '<ins>' + b_text + '</ins>'\n",
    "        if tag == 'replace':\n",
    "            # combine as delete + insert\n",
    "            return '<del>' + a_text + '</del><ins>' + b_text + '</ins>'\n",
    "        assert False, \"Unknown tag %r\"%tag\n",
    "    return ''.join(process_tag(*t) for t in matcher.get_opcodes())\n",
    "\n",
    "def display_diff(row):\n",
    "    diff_css = \"\"\"\n",
    "    <style>\n",
    "    ins {background-color: #aaffaa;}  // light green\n",
    "    del {background-color: #ffaaaa;}  // light red\n",
    "    repl {background-color: #bb99ff;} // light purple\n",
    "    table tr > * { width: 50%; }\n",
    "    table tr > td { vertical-align: top; text-align: left; }\n",
    "    </style>\n",
    "    \"\"\"\n",
    "\n",
    "    run_name = (\n",
    "        f\"{row.task} - {row.strategy} - {row.msid} - {row.run}\"\n",
    "        if row.task == \"extract_figure_legends\"\n",
    "        else f\"{row.task} - {row.strategy} - {row.msid} - {row.figure_label} - {row.run}\"\n",
    "    )\n",
    "    header = f\"<h3>{run_name}</h3>\"\n",
    "    section_fn = lambda title, content: f\"<h4>{title}</h4><p>{content}</p>\"\n",
    "    scores = section_fn(\"Scores\", f\"bleu1: {row.bleu1:.2f}, bleu2: {row.bleu2:.2f}, bleu3: {row.bleu3:.2f}, bleu4: {row.bleu4:.2f}, rouge1: {row.rouge1:.2f}, rouge2: {row.rouge2:.2f}, rougeL: {row.rougeL:.2f}\")\n",
    "\n",
    "    comparison = f\"\"\"\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Expected</th>\n",
    "            <th>Actual</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>{html.escape(row.expected)}</td>\n",
    "            <td>{html.escape(row.actual)}</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\"\"\"\n",
    "    diff = section_fn(\"Diff\", inline_diff(row.expected, row.actual) + comparison)\n",
    "    input_text = section_fn(\"Input\", row.input)\n",
    "\n",
    "    display(HTML(diff_css + header + scores + diff + input_text))\n",
    "\n",
    "def display_all(df):\n",
    "    print(f\"Showing {len(df)} runs\")\n",
    "    for row in df.itertuples():\n",
    "        display_diff(row)\n",
    "\n",
    "def show_runs(\n",
    "    df,\n",
    "    tasks=None,\n",
    "    strategies=None,\n",
    "    msids=None,\n",
    "    figure_labels=None,\n",
    "    runs=None,\n",
    "    score_range=None,\n",
    "):\n",
    "    if tasks is not None:\n",
    "        df = df[df.task.isin(tasks)]\n",
    "    if strategies is not None:\n",
    "        df = df[df.strategy.isin(strategies)]\n",
    "    if msids is not None:\n",
    "        df = df[df.msid.isin(msids)]\n",
    "    if figure_labels is not None:\n",
    "        df = df[df.figure_label.isin(figure_labels)]\n",
    "    if runs is not None:\n",
    "        df = df[df.run.isin(runs)]\n",
    "    if score_range is not None:\n",
    "        min_score, max_score = score_range\n",
    "        df = df[(df[score] >= min_score) & (df[score] < max_score)]\n",
    "    return display_all(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_runs(\n",
    "    df,\n",
    "    tasks=[\"extract_figure_caption\"],\n",
    "    strategies=[\"gpt-4o t=0 t_p=0\"],\n",
    "    msids=[\"EMBOJ-2023-114687\"],\n",
    "    figure_labels=[\"Figure 1\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_runs(\n",
    "    df,\n",
    "    tasks=[\"extract_figure_legends\"],\n",
    "    strategies=[\"openai\"],\n",
    "    score_range=(0., 0.9),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
