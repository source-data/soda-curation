{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "results_file = \"data/eval/results.json\"\n",
    "score = \"bleu1\"\n",
    "ignore = {\n",
    "    \"task\": None,\n",
    "    \"strategy\": [\"regex\"],\n",
    "    \"msid\": None,\n",
    "    \"figure_label\": None,\n",
    "    \"run\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(results_file)\n",
    "\n",
    "# remove runs in ignore\n",
    "for k, v in ignore.items():\n",
    "    if v is not None:\n",
    "        df = df[~df[k].isin(v)]\n",
    "\n",
    "# run is a number, but should be considered a category\n",
    "df = df.assign(run=df.run.astype('category'))\n",
    "\n",
    "n_tasks = df.task.nunique()\n",
    "n_strategies = df.strategy.nunique()\n",
    "n_msids = df.msid.nunique()\n",
    "n_figures = df.figure_label.nunique()\n",
    "n_runs = df.run.nunique()\n",
    "\n",
    "assert df[score].min() >= 0, \"Score should be non-negative\"\n",
    "assert df[score].max() <= 1, \"Score should be at most 1\"\n",
    "\n",
    "# split into tasks\n",
    "legends = df[df[\"task\"] == \"extract_figure_legends\"]\n",
    "labels = df[df[\"task\"] == \"extract_figures\"]\n",
    "titles = df[df[\"task\"] == \"extract_figure_title\"]\n",
    "captions = df[df[\"task\"] == \"extract_figure_caption\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-line stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"task\", \"strategy\"])[[score]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def add_threshold(df, fig, axis=\"y\"):\n",
    "    score_threshold = df[f\"{score}_threshold\"].iloc[0].round(2)\n",
    "    kwargs = dict(\n",
    "        line_dash=\"dash\",\n",
    "        line_color=\"green\",\n",
    "        opacity=0.5,\n",
    "        annotation_text=f\"threshold: {score_threshold}\",\n",
    "        annotation_position=\"top left\",\n",
    "    )\n",
    "    if axis == \"y\":\n",
    "        fig.add_hline(y=score_threshold, **kwargs)\n",
    "    else:\n",
    "        fig.add_vline(x=score_threshold, **kwargs)\n",
    "\n",
    "def hist_score(df, interval_width=0.01, **kwargs):\n",
    "    groupby = [\"task\", \"strategy\"]\n",
    "    x_start = 0 - interval_width\n",
    "    x_end = 1 + interval_width\n",
    "    y_start = -1\n",
    "    y_end = df.groupby(groupby).count().max().max() + 1\n",
    "    fig = px.histogram(\n",
    "        df,\n",
    "        x=score,\n",
    "        range_x=[x_start, x_end],\n",
    "        range_y=[y_start, y_end],\n",
    "        facet_col=\"task\",\n",
    "        facet_row=\"strategy\",\n",
    "        width=900,\n",
    "        height=100 + 250 * n_strategies,\n",
    "        **kwargs,\n",
    "    )\n",
    "    fig.update_traces(\n",
    "        xbins=dict( # bins used for histogram\n",
    "            start=x_start,\n",
    "            end=x_end,\n",
    "            size=interval_width,\n",
    "        )\n",
    "    )\n",
    "    fig.update_layout(bargap=0.2)\n",
    "    add_threshold(df, fig, axis=\"x\")\n",
    "    return fig\n",
    "\n",
    "def scatter_score(df, **kwargs):\n",
    "    fig = px.scatter(\n",
    "        df,\n",
    "        x=\"msid\",\n",
    "        y=score,\n",
    "        facet_col=\"task\",\n",
    "        facet_row=\"strategy\",\n",
    "        color=\"run\",\n",
    "        height=100 + 250 * n_strategies,\n",
    "        width=900,\n",
    "        **kwargs,\n",
    "    )\n",
    "    fig.update_traces(marker_size=10)\n",
    "    fig.update_layout(scattermode=\"group\", scattergap=0.9)\n",
    "    add_threshold(df, fig)\n",
    "    return fig\n",
    "\n",
    "def plot_std(df, groupby, **kwargs):\n",
    "    df_std = (\n",
    "        df.groupby(groupby)\n",
    "        [score].std()\n",
    "        .reset_index()\n",
    "        .sort_values(groupby)\n",
    "        .rename(columns={score: f\"{score}_std\"})\n",
    "    )\n",
    "    fig = px.scatter(\n",
    "        df_std,\n",
    "        x=\"msid\",\n",
    "        y=f\"{score}_std\",\n",
    "        facet_col=\"task\",\n",
    "        facet_row=\"strategy\",\n",
    "        height=100 + 250 * n_strategies,\n",
    "        width=900,\n",
    "        **kwargs,\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_df in [legends, labels]:\n",
    "    if task_df.empty:\n",
    "        continue\n",
    "    hist_score(task_df).show()\n",
    "    scatter_score(task_df).show()\n",
    "    plot_std(task_df, groupby=[\"task\", \"strategy\", \"msid\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_df in [titles, captions]:\n",
    "    if task_df.empty:\n",
    "        continue\n",
    "    hist_score(task_df).show()\n",
    "    scatter_score(\n",
    "        task_df,\n",
    "        symbol=\"figure_label\",\n",
    "    ).show()\n",
    "    plot_std(\n",
    "        task_df,\n",
    "        groupby=[\"task\", \"strategy\", \"msid\", \"figure_label\"],\n",
    "        color=\"figure_label\",\n",
    "        symbol=\"figure_label\",\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "import html\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def inline_diff(a, b):\n",
    "    matcher = difflib.SequenceMatcher(None, a, b)\n",
    "    def process_tag(tag, i1, i2, j1, j2):\n",
    "        a_text = html.escape(a[i1:i2])\n",
    "        b_text = html.escape(b[j1:j2])\n",
    "        if tag == 'delete':\n",
    "            return '<del>' + a_text + '</del>'\n",
    "        if tag == 'equal':\n",
    "            return a_text\n",
    "        if tag == 'insert':\n",
    "            return '<ins>' + b_text + '</ins>'\n",
    "        if tag == 'replace':\n",
    "            # combine as delete + insert\n",
    "            return '<del>' + a_text + '</del><ins>' + b_text + '</ins>'\n",
    "        assert False, \"Unknown tag %r\"%tag\n",
    "    return ''.join(process_tag(*t) for t in matcher.get_opcodes())\n",
    "\n",
    "def display_diff_(row):\n",
    "    diff_css = \"\"\"\n",
    "    <style>\n",
    "    ins {background-color: #588a41;}  // light green\n",
    "    del {background-color: #ffaaaa;}  // light red\n",
    "    repl {background-color: #bb99ff;} // light purple\n",
    "    table tr > * { width: 50%; }\n",
    "    table tr > td { vertical-align: top; text-align: left; }\n",
    "    </style>\n",
    "    \"\"\"\n",
    "\n",
    "    # run_name = (\n",
    "    #     f\"{row.task} - {row.strategy} - {row.msid} - {row.run}\"\n",
    "    #     if row.task == \"extract_figure_caption\"\n",
    "    #     else f\"{row.task} - {row.strategy} - {row.msid} - {row.figure_label} - {row.run}\"\n",
    "    # )\n",
    "    # header = f\"<h3>{run_name}</h3>\"\n",
    "    section_fn = lambda title, content: f\"<h4>{title}</h4><p>{content}</p>\"\n",
    "    # scores = section_fn(\"Scores\", f\"bleu1: {row.bleu1[0]:.2f}}\")\n",
    "\n",
    "    comparison = f\"\"\"\n",
    "        <table>\n",
    "            <thead>\n",
    "                <tr>\n",
    "                    <th>Expected</th>\n",
    "                    <th>Actual</th>\n",
    "                </tr>\n",
    "            </thead>\n",
    "            <tbody>\n",
    "                <tr>\n",
    "                    <td>{html.escape(row.head(1)[\"expected\"].values[0])}</td>\n",
    "                    <td>{html.escape(row.head(1)[\"actual\"].values[0])}</td>\n",
    "                </tr>\n",
    "            </tbody>\n",
    "        </table>\n",
    "    \"\"\"\n",
    "    diff = section_fn(\"Diff\", inline_diff(row.head(1)[\"expected\"].values[0], row.head(1)[\"actual\"].values[0]) + comparison)\n",
    "    input_text = section_fn(\"Input\", row.input)\n",
    "\n",
    "    # display(HTML(diff_css + header + scores + diff + input_text))\n",
    "    display(HTML(diff_css + diff + input_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"task\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bad_figure_legends = df[(df[\"task\"]==\"extract_figure_legends\") & (df[\"bleu1\"]<0.95)]\n",
    "df_bad_labels = df[(df[\"task\"]==\"extract_figures\") & (df[\"bleu1\"]<0.95)]\n",
    "df_bad_titles = df[(df[\"task\"]==\"extract_figure_title\") & (df[\"bleu1\"]<0.95)]\n",
    "df_bad_caption = df[(df[\"task\"]==\"extract_figure_caption\") & (df[\"bleu1\"]<0.95)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of bad detection of figure section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_diff_(df_bad_figure_legends.sample()) if len(df_bad_figure_legends) > 0 else print(\"No bad figure legends\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of bad detection of figure label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_diff_(df_bad_labels.sample()) if len(df_bad_labels) > 0 else print(\"No bad figure labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of bad figure titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_diff_(df_bad_titles.sample()) if len(df_bad_titles) > 0 else print(\"No bad figure titles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of bad figure captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_diff_(df_bad_caption.sample()) if len(df_bad_caption) > 0 else print(\"No bad figure captions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single visualizations where we could have a better intuition of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Create new column for model type (GPT vs Claude)\n",
    "df['model_type'] = df['strategy'].apply(lambda x: 'GPT' if 'gpt' in x.lower() else 'Claude' if 'claude' in x.lower() else 'Other')\n",
    "\n",
    "# Define consistent colors and bins\n",
    "colors = {'GPT': '#2E91E5', 'Claude': '#E15F99'}  # Blue for GPT, Pink for Claude\n",
    "\n",
    "# Create uniform bins from 0 to 1\n",
    "bin_size = 0.025  # This creates 20 bins of size 0.05 each\n",
    "bins = list(np.arange(0, 1.1, bin_size))  # Adding .05 to include 1.0\n",
    "\n",
    "# Create distribution plots per task\n",
    "tasks = df['task'].unique()\n",
    "fig_dist = make_subplots(\n",
    "    rows=len(tasks), \n",
    "    cols=1,\n",
    "    subplot_titles=[task.replace('_', ' ').title() for task in tasks],\n",
    "    vertical_spacing=0.1\n",
    ")\n",
    "\n",
    "for i, task in enumerate(tasks, 1):\n",
    "    task_data = df[df['model_type'].isin(['GPT', 'Claude']) & (df['task'] == task)]\n",
    "    \n",
    "    for model in ['GPT', 'Claude']:\n",
    "        model_data = task_data[task_data['model_type'] == model]['bleu1']\n",
    "        \n",
    "        # Create histogram trace with consistent bins\n",
    "        fig_dist.add_trace(\n",
    "            go.Histogram(\n",
    "                x=model_data,\n",
    "                name=model,\n",
    "                histfunc='count',\n",
    "                xbins=dict(\n",
    "                    start=0,\n",
    "                    end=1.1,\n",
    "                    size=bin_size\n",
    "                ),\n",
    "                hovertemplate=\"Score Range: %{x}<br>Count: %{y}<extra></extra>\",\n",
    "                cumulative_enabled=False,\n",
    "                showlegend=i==1,  # Only show legend for first subplot\n",
    "                opacity=0.7,\n",
    "                marker_color=colors[model],\n",
    "                marker_line_width=1,\n",
    "                marker_line_color='white',\n",
    "                histnorm='probability'\n",
    "            ),\n",
    "            row=i, \n",
    "            col=1\n",
    "        )\n",
    "\n",
    "# Update layout after creating all subplots\n",
    "fig_dist.update_layout(\n",
    "    title='Score Distribution by Task: GPT vs Claude',\n",
    "    width=900,\n",
    "    height=300*len(tasks),\n",
    "    barmode='overlay',\n",
    "    bargap=0.1\n",
    ")\n",
    "\n",
    "# Update each subplot's axes\n",
    "for i in range(len(tasks)):\n",
    "    fig_dist.update_yaxes(title_text=\"Probability\", row=i+1, col=1)\n",
    "    fig_dist.update_xaxes(\n",
    "        title_text=\"BLEU-1 Score\", \n",
    "        range=[0, 1.02],\n",
    "        dtick=0.1,  # Add gridlines every 0.1\n",
    "        row=i+1, \n",
    "        col=1\n",
    "    )\n",
    "\n",
    "fig_dist.show()\n",
    "\n",
    "# Bar plot with error bars - using the same colors\n",
    "fig_bar = px.bar(\n",
    "    model_stats,\n",
    "    x='task',\n",
    "    y='mean',\n",
    "    error_y='std',\n",
    "    color='model_type',\n",
    "    barmode='group',\n",
    "    title='Average Performance by Task: GPT vs Claude',\n",
    "    labels={\n",
    "        'task': 'Task',\n",
    "        'mean': 'Average BLEU-1 Score',\n",
    "        'model_type': 'Model Type'\n",
    "    },\n",
    "    color_discrete_map=colors\n",
    ")\n",
    "\n",
    "fig_bar.update_layout(\n",
    "    yaxis_range=[0, 1],\n",
    "    width=900,\n",
    "    height=500,\n",
    "    xaxis_tickangle=-45\n",
    ")\n",
    "fig_bar.show()\n",
    "\n",
    "# Print summary statistics and t-tests\n",
    "print(\"\\nSummary Statistics by Task:\")\n",
    "print(model_stats.round(3))\n",
    "\n",
    "print(\"\\nt-test results by task:\")\n",
    "for task in tasks:\n",
    "    task_data = df[df['task'] == task]\n",
    "    gpt_scores = task_data[task_data['model_type'] == 'GPT']['bleu1']\n",
    "    claude_scores = task_data[task_data['model_type'] == 'Claude']['bleu1']\n",
    "    t_stat, p_value = stats.ttest_ind(gpt_scores, claude_scores)\n",
    "    print(f\"\\nTask: {task}\")\n",
    "    print(f\"t-statistic: {t_stat:.3f}\")\n",
    "    print(f\"p-value: {p_value:.3e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's identify all GPT and Claude variations\n",
    "gpt_strategies = df[df['strategy'].str.contains('gpt', case=False)]['strategy'].unique()\n",
    "claude_strategies = df[df['strategy'].str.contains('claude', case=False)]['strategy'].unique()\n",
    "\n",
    "# Calculate stats for GPT variations\n",
    "gpt_stats = df[df['strategy'].isin(gpt_strategies)].groupby(['task', 'strategy'])['bleu1'].agg([\n",
    "    'mean',\n",
    "    'std',\n",
    "    'count'\n",
    "]).reset_index()\n",
    "\n",
    "# Calculate stats for Claude variations\n",
    "claude_stats = df[df['strategy'].isin(claude_strategies)].groupby(['task', 'strategy'])['bleu1'].agg([\n",
    "    'mean',\n",
    "    'std',\n",
    "    'count'\n",
    "]).reset_index()\n",
    "\n",
    "# Create bar plot for GPT variations\n",
    "fig_gpt = px.bar(\n",
    "    gpt_stats,\n",
    "    x='task',\n",
    "    y='mean',\n",
    "    error_y='std',\n",
    "    color='strategy',\n",
    "    barmode='group',\n",
    "    title='Performance by Task: GPT Variations',\n",
    "    labels={\n",
    "        'task': 'Task',\n",
    "        'mean': 'Average BLEU-1 Score',\n",
    "        'strategy': 'Strategy'\n",
    "    },\n",
    "    color_discrete_sequence=px.colors.qualitative.Set1  # Use a consistent color palette\n",
    ")\n",
    "\n",
    "fig_gpt.update_layout(\n",
    "    yaxis_range=[0, 1.2],\n",
    "    width=900,\n",
    "    height=500,\n",
    "    xaxis_tickangle=-45,\n",
    "    showlegend=True,\n",
    "    legend_title_text='GPT Variations'\n",
    ")\n",
    "fig_gpt.show()\n",
    "\n",
    "# Create bar plot for Claude variations\n",
    "fig_claude = px.bar(\n",
    "    claude_stats,\n",
    "    x='task',\n",
    "    y='mean',\n",
    "    error_y='std',\n",
    "    color='strategy',\n",
    "    barmode='group',\n",
    "    title='Performance by Task: Claude Variations',\n",
    "    labels={\n",
    "        'task': 'Task',\n",
    "        'mean': 'Average BLEU-1 Score',\n",
    "        'strategy': 'Strategy'\n",
    "    },\n",
    "    color_discrete_sequence=px.colors.qualitative.Set2  # Use a different color palette for Claude\n",
    ")\n",
    "\n",
    "fig_claude.update_layout(\n",
    "    yaxis_range=[0, 1.2],\n",
    "    width=900,\n",
    "    height=500,\n",
    "    xaxis_tickangle=-45,\n",
    "    showlegend=True,\n",
    "    legend_title_text='Claude Variations'\n",
    ")\n",
    "fig_claude.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nSummary Statistics for GPT Variations:\")\n",
    "print(gpt_stats.round(3))\n",
    "print(\"\\nSummary Statistics for Claude Variations:\")\n",
    "print(claude_stats.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's get our distributions per manuscript\n",
    "# Create distribution plots per manuscript\n",
    "manuscripts = df['msid'].unique()\n",
    "fig_dist = make_subplots(\n",
    "    rows=len(manuscripts), \n",
    "    cols=1,\n",
    "    subplot_titles=[f\"Manuscript {msid}\" for msid in manuscripts],\n",
    "    vertical_spacing=0.1\n",
    ")\n",
    "\n",
    "# Define consistent colors and bins\n",
    "colors = {'GPT': '#2E91E5', 'Claude': '#E15F99'}\n",
    "bin_size = 0.05\n",
    "bins = list(np.arange(0, 1.05, bin_size))\n",
    "\n",
    "for i, msid in enumerate(manuscripts, 1):\n",
    "    manuscript_data = df[df['model_type'].isin(['GPT', 'Claude']) & (df['msid'] == msid)]\n",
    "    \n",
    "    for model in ['GPT', 'Claude']:\n",
    "        model_data = manuscript_data[manuscript_data['model_type'] == model]['bleu1']\n",
    "        \n",
    "        fig_dist.add_trace(\n",
    "            go.Histogram(\n",
    "                x=model_data,\n",
    "                name=model,\n",
    "                histfunc='count',\n",
    "                xbins=dict(\n",
    "                    start=0,\n",
    "                    end=1,\n",
    "                    size=bin_size\n",
    "                ),\n",
    "                hovertemplate=\"Score Range: %{x}<br>Count: %{y}<extra></extra>\",\n",
    "                cumulative_enabled=False,\n",
    "                showlegend=i==1,\n",
    "                opacity=0.7,\n",
    "                marker_color=colors[model],\n",
    "                marker_line_width=1,\n",
    "                marker_line_color='white',\n",
    "                histnorm='probability'\n",
    "            ),\n",
    "            row=i, \n",
    "            col=1\n",
    "        )\n",
    "\n",
    "fig_dist.update_layout(\n",
    "    title='Score Distribution by Manuscript: GPT vs Claude',\n",
    "    width=900,\n",
    "    height=300*len(manuscripts),\n",
    "    barmode='overlay',\n",
    "    bargap=0.1\n",
    ")\n",
    "\n",
    "for i in range(len(manuscripts)):\n",
    "    fig_dist.update_yaxes(title_text=\"Probability\", row=i+1, col=1)\n",
    "    fig_dist.update_xaxes(\n",
    "        title_text=\"BLEU-1 Score\", \n",
    "        range=[0, 1.1],\n",
    "        dtick=0.1,\n",
    "        row=i+1, \n",
    "        col=1\n",
    "    )\n",
    "\n",
    "fig_dist.show()\n",
    "\n",
    "# Now let's analyze GPT and Claude variations per manuscript\n",
    "# For GPT variations\n",
    "gpt_manuscript_stats = df[df['strategy'].isin(gpt_strategies)].groupby(['msid', 'strategy'])['bleu1'].agg([\n",
    "    'mean',\n",
    "    'std',\n",
    "    'count'\n",
    "]).reset_index()\n",
    "\n",
    "fig_gpt_manuscript = px.bar(\n",
    "    gpt_manuscript_stats,\n",
    "    x='msid',\n",
    "    y='mean',\n",
    "    error_y='std',\n",
    "    color='strategy',\n",
    "    barmode='group',\n",
    "    title='Performance by Manuscript: GPT Variations',\n",
    "    labels={\n",
    "        'msid': 'Manuscript ID',\n",
    "        'mean': 'Average BLEU-1 Score',\n",
    "        'strategy': 'Strategy'\n",
    "    },\n",
    "    color_discrete_sequence=px.colors.qualitative.Set1\n",
    ")\n",
    "\n",
    "fig_gpt_manuscript.update_layout(\n",
    "    yaxis_range=[0, 1.1],\n",
    "    width=900,\n",
    "    height=500,\n",
    "    xaxis_tickangle=-45,\n",
    "    showlegend=True,\n",
    "    legend_title_text='GPT Variations'\n",
    ")\n",
    "fig_gpt_manuscript.show()\n",
    "\n",
    "# For Claude variations\n",
    "claude_manuscript_stats = df[df['strategy'].isin(claude_strategies)].groupby(['msid', 'strategy'])['bleu1'].agg([\n",
    "    'mean',\n",
    "    'std',\n",
    "    'count'\n",
    "]).reset_index()\n",
    "\n",
    "fig_claude_manuscript = px.bar(\n",
    "    claude_manuscript_stats,\n",
    "    x='msid',\n",
    "    y='mean',\n",
    "    error_y='std',\n",
    "    color='strategy',\n",
    "    barmode='group',\n",
    "    title='Performance by Manuscript: Claude Variations',\n",
    "    labels={\n",
    "        'msid': 'Manuscript ID',\n",
    "        'mean': 'Average BLEU-1 Score',\n",
    "        'strategy': 'Strategy'\n",
    "    },\n",
    "    color_discrete_sequence=px.colors.qualitative.Set2\n",
    ")\n",
    "\n",
    "fig_claude_manuscript.update_layout(\n",
    "    yaxis_range=[0, 1.1],\n",
    "    width=900,\n",
    "    height=500,\n",
    "    xaxis_tickangle=-45,\n",
    "    showlegend=True,\n",
    "    legend_title_text='Claude Variations'\n",
    ")\n",
    "fig_claude_manuscript.show()\n",
    "\n",
    "# Let's also analyze performance per manuscript and task\n",
    "manuscript_task_stats = df[df['model_type'].isin(['GPT', 'Claude'])].groupby(['msid', 'task', 'model_type'])['bleu1'].agg([\n",
    "    'mean',\n",
    "    'std',\n",
    "    'count'\n",
    "]).reset_index()\n",
    "\n",
    "fig_task_manuscript = px.bar(\n",
    "    manuscript_task_stats,\n",
    "    x='msid',\n",
    "    y='mean',\n",
    "    error_y='std',\n",
    "    color='model_type',\n",
    "    facet_row='task',\n",
    "    barmode='group',\n",
    "    title='Performance by Manuscript and Task: GPT vs Claude',\n",
    "    labels={\n",
    "        'msid': 'Manuscript ID',\n",
    "        'mean': 'Average BLEU-1 Score',\n",
    "        'model_type': 'Model Type'\n",
    "    },\n",
    "    color_discrete_map=colors\n",
    ")\n",
    "\n",
    "fig_task_manuscript.update_layout(\n",
    "    yaxis_range=[0, 1.1],\n",
    "    width=900,\n",
    "    height=200*len(df['task'].unique()),\n",
    "    showlegend=True\n",
    ")\n",
    "fig_task_manuscript.show()\n",
    "\n",
    "# Print summary statistics for potentially problematic manuscripts\n",
    "print(\"\\nIdentifying potentially problematic manuscripts:\")\n",
    "for msid in manuscripts:\n",
    "    manuscript_data = df[df['msid'] == msid]\n",
    "    mean_score = manuscript_data['bleu1'].mean()\n",
    "    std_score = manuscript_data['bleu1'].std()\n",
    "    \n",
    "    if mean_score < df['bleu1'].mean() - df['bleu1'].std():\n",
    "        print(f\"\\nManuscript {msid} shows lower than average performance:\")\n",
    "        print(f\"Mean score: {mean_score:.3f}\")\n",
    "        print(f\"Std deviation: {std_score:.3f}\")\n",
    "        \n",
    "        # Show task-specific performance for this manuscript\n",
    "        task_performance = manuscript_data.groupby('task')['bleu1'].agg(['mean', 'std'])\n",
    "        print(\"\\nTask-specific performance:\")\n",
    "        print(task_performance.round(3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
