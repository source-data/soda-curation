{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [12]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SODA Curation Benchmarking Report\n",
    "\n",
    "## Overview\n",
    "This report analyzes the performance of different AI strategies on the SODA Curation tasks. The strategies include variations of GPT and Claude models with different parameters.\n",
    "\n",
    "## Tasks Performance Analysis\n",
    "\n",
    "### Score Distributions\n",
    "The following plots show the distribution of scores across different strategies for each task. The step histograms reveal:\n",
    "\n",
    "- Density of scores in different ranges (0-1)\n",
    "- Relative performance between strategies\n",
    "- Potential bimodal patterns or clusters\n",
    "\n",
    "### Box Plots \n",
    "Box plots complement the histograms by showing:\n",
    "\n",
    "- Median performance\n",
    "- Score spread (IQR)\n",
    "- Outliers\n",
    "- Direct strategy comparisons\n",
    "\n",
    "## Detailed Results\n",
    "\n",
    "{Performance plots will be added here}\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "- Task-specific performance patterns\n",
    "- Strategy comparisons\n",
    "- Areas for improvement\n",
    "- Notable success cases\n",
    "\n",
    "## Technical Details\n",
    "- Score metrics: BLEU-1, ROUGE scores\n",
    "- Data collection period: {date}\n",
    "- Number of test cases per task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy import stats\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define color palette\n",
    "PALETTE = {\n",
    "    \"primary_yellow\": \"#F2B80E\",  # Primary\n",
    "    \"primary_black\":  \"#222222\",  # Primary\n",
    "    \"secondary_red\":  \"#C53B2E\",  # Secondary\n",
    "    \"secondary_orange\": \"#E1812B\",# Secondary\n",
    "    \"secondary_green\": \"#94AF1F\", # Secondary\n",
    "    \"tertiary_blue\":   \"#449EAF\", # Tertiary\n",
    "    \"tertiary_purple\": \"#3D2D81\", # Tertiary\n",
    "    \"tertiary_teal\":   \"#43806E\", # Tertiary\n",
    "    \"tertiary_magenta\":\"#8E2D64\"  # Tertiary\n",
    "}\n",
    "\n",
    "\n",
    "# Create figures directory\n",
    "figures_dir = Path(\"./figures\")\n",
    "figures_dir.mkdir(exist_ok=True)\n",
    "\n",
    "def ensure_figures_dir():\n",
    "    \"\"\"Create figures directory if it doesn't exist.\"\"\"\n",
    "    figures_dir = Path(\"./figures\")\n",
    "    figures_dir.mkdir(exist_ok=True)\n",
    "    return figures_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"results.json\")\n",
    "\n",
    "score = \"bleu1\"\n",
    "ignore = {\n",
    "    \"task\": None,\n",
    "    \"strategy\": [\"regex\"],\n",
    "    \"msid\": None,\n",
    "    \"figure_label\": None,\n",
    "    \"run\": None,\n",
    "}\n",
    "# remove runs in ignore\n",
    "for k, v in ignore.items():\n",
    "    if v is not None:\n",
    "        df = df[~df[k].isin(v)]\n",
    "\n",
    "# run is a number, but should be considered a category\n",
    "df = df.assign(run=df.run.astype('category'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_performance_plots(df, save_dir='figures'):\n",
    "    \"\"\"Create individual performance plots per task.\"\"\"\n",
    "    tasks = df['task'].unique()\n",
    "    strategies = df['strategy'].unique()\n",
    "    colors = list(PALETTE.values())[:len(strategies)]\n",
    "    \n",
    "    for task in tasks:\n",
    "        task_data = df[df['task'] == task]\n",
    "        \n",
    "        # Select appropriate score column\n",
    "        if task == 'panel_source_assignment':\n",
    "            score_col = 'panel_source_manuscript_accuracy_exact'\n",
    "        elif task == 'extract_data_sources':\n",
    "            score_col = 'data_source_accuracy_exact'\n",
    "        else:\n",
    "            score_col = 'bleu1'\n",
    "            \n",
    "        # Histogram plot\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        bins = np.linspace(0, 1, 21)  # 20 bins of equal width from 0 to 1\n",
    "        for strategy, color in zip(strategies, colors):\n",
    "            strategy_data = task_data[task_data['strategy'] == strategy][score_col]\n",
    "            plt.hist(strategy_data, bins=bins, histtype='step', label=strategy, \n",
    "                    color=color, linewidth=2, density=False)\n",
    "        \n",
    "        plt.title(f'Score Distribution for {task}')\n",
    "        plt.xlabel('Score')\n",
    "        plt.ylabel('Count')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(save_dir, f'{task}_histogram.pdf'))\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        # Box plot with filled boxes\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        bp = plt.boxplot([task_data[task_data['strategy'] == s][score_col] \n",
    "                            for s in strategies], labels=strategies, patch_artist=True)\n",
    "        \n",
    "        for box, color in zip(bp['boxes'], colors):\n",
    "            box.set_facecolor(color)\n",
    "            box.set_alpha(0.6)\n",
    "            box.set_edgecolor('black')\n",
    "            \n",
    "        plt.title(f'Score Distribution by Strategy for {task}')\n",
    "        plt.ylabel('Score')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(save_dir, f'{task}_boxplot.pdf'))\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    return \"Plots saved in figures directory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_performance_plots(df, save_dir='figures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_manuscript_performance_plots(df, save_dir='figures'):\n",
    "   \"\"\"Create performance plots showing aggregate scores per manuscript.\"\"\"\n",
    "   manuscripts = df['msid'].unique()\n",
    "   tasks = df['task'].unique()\n",
    "   \n",
    "   for task in tasks:\n",
    "       task_data = df[df['task'] == task]\n",
    "       score_col = ('panel_source_manuscript_accuracy_exact' if task == 'panel_source_assignment'\n",
    "                   else 'data_source_accuracy_exact' if task == 'extract_data_sources'\n",
    "                   else 'bleu1')       \n",
    "       # Box plot per manuscript\n",
    "       plt.figure(figsize=(12, 6))\n",
    "       bp = plt.boxplot([task_data[task_data['msid'] == m][score_col] \n",
    "                       for m in manuscripts], labels=manuscripts, patch_artist=True)\n",
    "       \n",
    "       for box, color in zip(bp['boxes'], PALETTE.values()):\n",
    "           box.set_facecolor(color)\n",
    "           box.set_alpha(0.6)\n",
    "           box.set_edgecolor('black')\n",
    "           \n",
    "       plt.title(f'Score Distribution by Manuscript for {task}')\n",
    "       plt.ylabel('Score')\n",
    "       plt.grid(True, alpha=0.3)\n",
    "       plt.xticks(rotation=45)\n",
    "       plt.tight_layout()\n",
    "       plt.savefig(os.path.join(save_dir, f'{task}_manuscript_boxplot.pdf'))\n",
    "       plt.show()\n",
    "       plt.close()\n",
    "\n",
    "   return \"Plots saved in figures directory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_manuscript_performance_plots(df, save_dir='figures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_task_errors_html(df, task_name, threshold=0.8, n_examples=3):\n",
    "   \"\"\"Display examples of errors for a given task in HTML format.\"\"\"\n",
    "   from IPython.display import HTML\n",
    "   import difflib\n",
    "   \n",
    "   task_data = df[df['task'] == task_name]\n",
    "   score_col = ('panel_source_manuscript_accuracy_exact' if task_name == 'panel_source_assignment'\n",
    "               else 'data_source_accuracy_exact' if task_name == 'extract_data_sources'\n",
    "               else 'bleu1')\n",
    "   \n",
    "   errors = task_data[task_data[score_col] < threshold].sort_values(score_col)\n",
    "   \n",
    "   if len(errors) == 0:\n",
    "       return HTML(\"<p>No examples found below threshold {threshold}</p>\")\n",
    "   \n",
    "   html_output = \"\"\"\n",
    "   <style>\n",
    "       .error-container { margin: 20px 0; padding: 15px; border: 1px solid #ddd; }\n",
    "       .error-header { font-weight: bold; margin-bottom: 10px; }\n",
    "       .comparison-container { display: flex; margin: 10px 0; }\n",
    "       .text-column { flex: 1; padding: 10px; border: 1px solid #eee; margin: 0 5px; }\n",
    "       .diff-view { margin-top: 10px; padding: 10px; background-color: #f8f8f8; }\n",
    "       .diff-add { background-color: #e6ffe6; }\n",
    "       .diff-del { background-color: #ffe6e6; }\n",
    "   </style>\n",
    "   \"\"\"\n",
    "   \n",
    "   for _, row in errors.head(n_examples).iterrows():\n",
    "       html_output += f\"\"\"\n",
    "       <div class=\"error-container\">\n",
    "           <div class=\"error-header\">\n",
    "               Score: {row[score_col]:.3f} | Strategy: {row['strategy']} | Manuscript: {row['msid']}\n",
    "           </div>\n",
    "           <div class=\"comparison-container\">\n",
    "               <div class=\"text-column\">\n",
    "                   <strong>Expected:</strong><br>\n",
    "                   {row['expected']}\n",
    "               </div>\n",
    "               <div class=\"text-column\">\n",
    "                   <strong>Actual:</strong><br>\n",
    "                   {row['actual']}\n",
    "               </div>\n",
    "           </div>\n",
    "           <div class=\"diff-view\">\n",
    "               <strong>Differences:</strong><br>\n",
    "       \"\"\"\n",
    "       \n",
    "       # Generate diff\n",
    "       d = difflib.HtmlDiff()\n",
    "       diff_html = d.make_table(row['expected'].splitlines(), \n",
    "                              row['actual'].splitlines(), \n",
    "                              'Expected', 'Actual',\n",
    "                              context=True)\n",
    "       html_output += diff_html + \"</div></div>\"\n",
    "   \n",
    "   return HTML(html_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_task_errors_html(df, \"extract_figure_caption\", threshold=0.8, n_examples=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_task_errors_html(df, \"extract_figure_title\", threshold=0.8, n_examples=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_task_errors_html(df, \"extract_data_availability_section\", threshold=0.8, n_examples=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_task_errors_html(df, \"panel_source_assignment\", threshold=0.8, n_examples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_task_errors_html(df, \"extract_data_sources\", threshold=0.8, n_examples=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
