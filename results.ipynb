{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "results_file = \"data/eval/results.json\"\n",
    "score = \"bleu1\"\n",
    "ignore = {\n",
    "    \"task\": None,\n",
    "    \"strategy\": [\"regex\"],\n",
    "    \"msid\": None,\n",
    "    \"figure_label\": None,\n",
    "    \"run\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(results_file)\n",
    "\n",
    "# remove runs in ignore\n",
    "for k, v in ignore.items():\n",
    "    if v is not None:\n",
    "        df = df[~df[k].isin(v)]\n",
    "\n",
    "# run is a number, but should be considered a category\n",
    "df = df.assign(run=df.run.astype('category'))\n",
    "\n",
    "n_tasks = df.task.nunique()\n",
    "n_strategies = df.strategy.nunique()\n",
    "n_msids = df.msid.nunique()\n",
    "n_figures = df.figure_label.nunique()\n",
    "n_runs = df.run.nunique()\n",
    "\n",
    "assert df[score].min() >= 0, \"Score should be non-negative\"\n",
    "assert df[score].max() <= 1, \"Score should be at most 1\"\n",
    "\n",
    "# split into tasks\n",
    "legends = df[df[\"task\"] == \"extract_figure_legends\"]\n",
    "labels = df[df[\"task\"] == \"extract_figures\"]\n",
    "titles = df[df[\"task\"] == \"extract_figure_title\"]\n",
    "captions = df[df[\"task\"] == \"extract_figure_caption\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-line stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"task\", \"strategy\"])[[score]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def add_threshold(df, fig, axis=\"y\"):\n",
    "    score_threshold = df[f\"{score}_threshold\"].iloc[0].round(2)\n",
    "    kwargs = dict(\n",
    "        line_dash=\"dash\",\n",
    "        line_color=\"green\",\n",
    "        opacity=0.5,\n",
    "        annotation_text=f\"threshold: {score_threshold}\",\n",
    "        annotation_position=\"top left\",\n",
    "    )\n",
    "    if axis == \"y\":\n",
    "        fig.add_hline(y=score_threshold, **kwargs)\n",
    "    else:\n",
    "        fig.add_vline(x=score_threshold, **kwargs)\n",
    "\n",
    "def hist_score(df, interval_width=0.01, **kwargs):\n",
    "    groupby = [\"task\", \"strategy\"]\n",
    "    x_start = 0 - interval_width\n",
    "    x_end = 1 + interval_width\n",
    "    y_start = -1\n",
    "    y_end = df.groupby(groupby).count().max().max() + 1\n",
    "    fig = px.histogram(\n",
    "        df,\n",
    "        x=score,\n",
    "        range_x=[x_start, x_end],\n",
    "        range_y=[y_start, y_end],\n",
    "        facet_col=\"task\",\n",
    "        facet_row=\"strategy\",\n",
    "        width=900,\n",
    "        height=100 + 250 * n_strategies,\n",
    "        **kwargs,\n",
    "    )\n",
    "    fig.update_traces(\n",
    "        xbins=dict( # bins used for histogram\n",
    "            start=x_start,\n",
    "            end=x_end,\n",
    "            size=interval_width,\n",
    "        )\n",
    "    )\n",
    "    fig.update_layout(bargap=0.2)\n",
    "    add_threshold(df, fig, axis=\"x\")\n",
    "    return fig\n",
    "\n",
    "def scatter_score(df, **kwargs):\n",
    "    fig = px.scatter(\n",
    "        df,\n",
    "        x=\"msid\",\n",
    "        y=score,\n",
    "        facet_col=\"task\",\n",
    "        facet_row=\"strategy\",\n",
    "        color=\"run\",\n",
    "        height=100 + 250 * n_strategies,\n",
    "        width=900,\n",
    "        **kwargs,\n",
    "    )\n",
    "    fig.update_traces(marker_size=10)\n",
    "    fig.update_layout(scattermode=\"group\", scattergap=0.9)\n",
    "    add_threshold(df, fig)\n",
    "    return fig\n",
    "\n",
    "def plot_std(df, groupby, **kwargs):\n",
    "    df_std = (\n",
    "        df.groupby(groupby)\n",
    "        [score].std()\n",
    "        .reset_index()\n",
    "        .sort_values(groupby)\n",
    "        .rename(columns={score: f\"{score}_std\"})\n",
    "    )\n",
    "    fig = px.scatter(\n",
    "        df_std,\n",
    "        x=\"msid\",\n",
    "        y=f\"{score}_std\",\n",
    "        facet_col=\"task\",\n",
    "        facet_row=\"strategy\",\n",
    "        height=100 + 250 * n_strategies,\n",
    "        width=900,\n",
    "        **kwargs,\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_df in [legends, labels]:\n",
    "    if task_df.empty:\n",
    "        continue\n",
    "    hist_score(task_df).show()\n",
    "    scatter_score(task_df).show()\n",
    "    plot_std(task_df, groupby=[\"task\", \"strategy\", \"msid\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_df in [titles, captions]:\n",
    "    if task_df.empty:\n",
    "        continue\n",
    "    hist_score(task_df).show()\n",
    "    scatter_score(\n",
    "        task_df,\n",
    "        symbol=\"figure_label\",\n",
    "    ).show()\n",
    "    plot_std(\n",
    "        task_df,\n",
    "        groupby=[\"task\", \"strategy\", \"msid\", \"figure_label\"],\n",
    "        color=\"figure_label\",\n",
    "        symbol=\"figure_label\",\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "import html\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def inline_diff(a, b):\n",
    "    matcher = difflib.SequenceMatcher(None, a, b)\n",
    "    def process_tag(tag, i1, i2, j1, j2):\n",
    "        a_text = html.escape(a[i1:i2])\n",
    "        b_text = html.escape(b[j1:j2])\n",
    "        if tag == 'delete':\n",
    "            return '<del>' + a_text + '</del>'\n",
    "        if tag == 'equal':\n",
    "            return a_text\n",
    "        if tag == 'insert':\n",
    "            return '<ins>' + b_text + '</ins>'\n",
    "        if tag == 'replace':\n",
    "            # combine as delete + insert\n",
    "            return '<del>' + a_text + '</del><ins>' + b_text + '</ins>'\n",
    "        assert False, \"Unknown tag %r\"%tag\n",
    "    return ''.join(process_tag(*t) for t in matcher.get_opcodes())\n",
    "\n",
    "def display_diff(row):\n",
    "    diff_css = \"\"\"\n",
    "    <style>\n",
    "    ins {background-color: #aaffaa;}  // light green\n",
    "    del {background-color: #ffaaaa;}  // light red\n",
    "    repl {background-color: #bb99ff;} // light purple\n",
    "    table tr > * { width: 50%; }\n",
    "    table tr > td { vertical-align: top; text-align: left; }\n",
    "    </style>\n",
    "    \"\"\"\n",
    "\n",
    "    run_name = (\n",
    "        f\"{row.task} - {row.strategy} - {row.msid} - {row.run}\"\n",
    "        if row.task == \"extract_figure_legends\"\n",
    "        else f\"{row.task} - {row.strategy} - {row.msid} - {row.figure_label} - {row.run}\"\n",
    "    )\n",
    "    header = f\"<h3>{run_name}</h3>\"\n",
    "    section_fn = lambda title, content: f\"<h4>{title}</h4><p>{content}</p>\"\n",
    "    scores = section_fn(\"Scores\", f\"bleu1: {row.bleu1:.2f}, bleu2: {row.bleu2:.2f}, bleu3: {row.bleu3:.2f}, bleu4: {row.bleu4:.2f}, rouge1: {row.rouge1:.2f}, rouge2: {row.rouge2:.2f}, rougeL: {row.rougeL:.2f}\")\n",
    "\n",
    "    comparison = f\"\"\"\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Expected</th>\n",
    "            <th>Actual</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>{html.escape(row.expected)}</td>\n",
    "            <td>{html.escape(row.actual)}</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\"\"\"\n",
    "    diff = section_fn(\"Diff\", inline_diff(row.expected, row.actual) + comparison)\n",
    "    input_text = section_fn(\"Input\", row.input)\n",
    "\n",
    "    display(HTML(diff_css + header + scores + diff + input_text))\n",
    "\n",
    "def display_all(df):\n",
    "    print(f\"Showing {len(df)} runs\")\n",
    "    for row in df.itertuples():\n",
    "        display_diff(row)\n",
    "\n",
    "def show_runs(\n",
    "    df,\n",
    "    tasks=None,\n",
    "    strategies=None,\n",
    "    msids=None,\n",
    "    figure_labels=None,\n",
    "    runs=None,\n",
    "    score_range=None,\n",
    "):\n",
    "    if tasks is not None:\n",
    "        df = df[df.task.isin(tasks)]\n",
    "    if strategies is not None:\n",
    "        df = df[df.strategy.isin(strategies)]\n",
    "    if msids is not None:\n",
    "        df = df[df.msid.isin(msids)]\n",
    "    if figure_labels is not None:\n",
    "        df = df[df.figure_label.isin(figure_labels)]\n",
    "    if runs is not None:\n",
    "        df = df[df.run.isin(runs)]\n",
    "    if score_range is not None:\n",
    "        min_score, max_score = score_range\n",
    "        df = df[(df[score] >= min_score) & (df[score] < max_score)]\n",
    "    return display_all(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_df in [legends, labels, titles, captions]:\n",
    "    if task_df.empty:\n",
    "        continue\n",
    "    print(f\"Task: {task_df.task.iloc[0]}\")\n",
    "    show_runs(task_df, score_range=(0., 0.949))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soda-curation-H-EFKjGu-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
