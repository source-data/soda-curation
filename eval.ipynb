{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "base_dir = Path(\"data\")\n",
    "\n",
    "archives_dir = base_dir / \"archives\"\n",
    "ground_truth_dir = base_dir / \"ground_truth\"\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "working_dir = base_dir / \"eval\" / timestamp\n",
    "\n",
    "for d in [archives_dir, ground_truth_dir, working_dir]:\n",
    "    d.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.soda_curation.eval import run_and_score_tasks, Task, Strategy\n",
    "\n",
    "n_repeats = 1\n",
    "tasks = [\n",
    "    Task.FIGURE_LEGEND,\n",
    "    Task.FIGURE_CAPTIONS,\n",
    "]\n",
    "strategies = [\n",
    "    # Strategy.CLAUDE,\n",
    "    Strategy.OPENAI,\n",
    "    Strategy.REGEX,\n",
    "]\n",
    "\n",
    "results = run_and_score_tasks(archives_dir, ground_truth_dir, working_dir, n_repeats=n_repeats, tasks=tasks, strategies=strategies)\n",
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([\n",
    "    {\n",
    "        \"task\": r[\"task\"],\n",
    "        \"strategy\": r[\"strategy\"],\n",
    "        \"msid\": r[\"msid\"],\n",
    "        \"run_id\": r[\"run_id\"],\n",
    "        \"accuracy\": r[\"score\"][\"accuracy\"],\n",
    "        \"precision\": r[\"score\"][\"precision\"],\n",
    "        \"recall\": r[\"score\"][\"recall\"],\n",
    "        \"f1\": r[\"score\"][\"f1\"],\n",
    "    }\n",
    "    for r in results\n",
    "])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _diffs_to_md_table(diffs):\n",
    "    md_table = [\n",
    "        \"| Type | Ground Truth | Extracted |\",\n",
    "        \"| --- | --- | --- |\",\n",
    "    ]\n",
    "    for diff in diffs:\n",
    "        a = diff[\"a\"].replace(\"\\n\", \"\\\\n\").replace(\"|\", \"\\|\")\n",
    "        b = diff[\"b\"].replace(\"\\n\", \"\\\\n\").replace(\"|\", \"\\|\")\n",
    "        type = diff[\"type\"]\n",
    "        md_table.append(f\"| {type} | {a} | {b} |\")\n",
    "    return md_table\n",
    "\n",
    "def to_md(results):\n",
    "    md = []\n",
    "    results_by_msid = {}\n",
    "    for r in results:\n",
    "        results_by_msid.setdefault(r[\"msid\"], []).append(r)\n",
    "    md.append(\"# Evaluation Results\")\n",
    "    md.append(\"## Summary\")\n",
    "    \n",
    "    md.append(df.groupby([\"task\", \"strategy\"]).describe()[[\"precision\", \"recall\", \"f1\", \"accuracy\"]].T.to_markdown())\n",
    "    md.append(df.groupby([\"task\", \"strategy\", \"msid\"]).describe()[[\"precision\", \"recall\", \"f1\", \"accuracy\"]].T.to_markdown())\n",
    "    for msid, ms_results in results_by_msid.items():\n",
    "        md.append(f\"## Manuscript: {msid}\")\n",
    "        for result in ms_results:\n",
    "            task = result[\"task\"]\n",
    "            strategy = result[\"strategy\"]\n",
    "            score = result[\"score\"]\n",
    "            md.append(f\"### Task: {task}, Strategy: {strategy}\")\n",
    "            md.append(f\"**F1 Score**: {score['f1']:.2f}\")\n",
    "            md.append(f\"**Precision**: {score['precision']:.2f}\")\n",
    "            md.append(f\"**Recall**: {score['recall']:.2f}\")\n",
    "            md.append(f\"**Accuracy**: {score['accuracy']:.2f}\")\n",
    "            md.append(f\"**TP**: {score['tp']}\")\n",
    "            md.append(f\"**FP**: {score['fp']}\")\n",
    "            md.append(f\"**FN**: {score['fn']}\")\n",
    "            md.append(f\"**Diff**:\")\n",
    "            md.extend(_diffs_to_md_table(score[\"diff\"]))\n",
    "\n",
    "    return \"\\n\".join(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(working_dir / \"results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2, sort_keys=True)\n",
    "\n",
    "with open(working_dir / \"results.md\", \"w\") as f:\n",
    "    f.write(to_md(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"task\", \"strategy\", \"msid\"]).describe()[[\"precision\", \"recall\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
