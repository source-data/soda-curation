{
  "manuscript_id": "MSB-2023-12087",
  "xml": "MSB-2023-12087.xml",
  "docx": "doc/Munck3.docx",
  "pdf": "pdf/MSB-2023-12087.pdf",
  "appendix": ["suppl_data/EV Figures Source Data.zip"],
  "figures": [
    {
      "figure_label": "Figure 1",
      "img_files": ["graphic/Figure1.pdf"],
      "sd_files": [],
      "figure_caption": "A) Illustration of Hamming code for error correction in data\ntransmission. Transmission of data (d) and parity (p) bits\nenables error correction via redundancy\n(https://en.wikipedia.org/wiki/Hamming_code). B) Diagram showing\nthe different sources of metadata information and how to bundle\nthem. Three independent resources – the electronic labnotebook,\nthe data-associated metadata, and the publication – are shown as\nredundant entries. An AI language model can be used to extract required\nand standardized data elements for verification, using codewords as a\nmeans of error correction analogous to error correction in\ncommunication. C) Heatmap display of similarities between\nsources by keyword. A Jupyter notebook using GPT-4 has been\nused to create a structured output in the form of a CSV file, (see Table\n1). The digestion of a labnotebook entry, a metadata file server file,\nand this manuscript are used to check for keywords. The consistency of\nthe keywords across the sources is displayed in a heatmap using the\ncosine distance for semantic similarity estimation <a\nhref=\"https://spacy.io/api/doc\">(<u>https://spacy.io/api/doc</u>)</a>.",
      "caption_title": "Linking metadata sources and digesting them with\nlanguage models to generate structured outputs and representations of\nsimilarity.",
      "panels": [
        {
          "panel_label": "A",
          "panel_caption": "Illustration of Hamming code for error correction in data transmission. Transmission of data (d) and parity (p) bits enables error correction via redundancy (https://en.wikipedia.org/wiki/Hamming_code).",
          "sd_files": []
        },
        {
          "panel_label": "B",
          "panel_caption": "Diagram showing the different sources of metadata information and how to bundle them. Three independent resources – the electronic labnotebook, the data-associated metadata, and the publication – are shown as redundant entries. An AI language model can be used to extract required and standardized data elements for verification, using codewords as a means of error correction analogous to error correction in communication. ",
          "sd_files": []
        },
        {
          "panel_label": "C",
          "panel_caption": "Heatmap display of similarities between sources by keyword. A Jupyter notebook using GPT-4 has been used to create a structured output in the form of a CSV file, (see Table 1). The digestion of a labnotebook entry, a metadata file server file, and this manuscript are used to check for keywords. The consistency of the keywords across the sources is displayed in a heatmap using the cosine distance for semantic similarity estimation (https://spacy.io/api/doc).",
          "sd_files": []
        }
      ]
    }
  ],
  "data_availability": {
    "section_text": "This study includes no data deposited in external repositories.",
    "data_sources": []
  },
  "all_captions": "Figure 1. Linking metadata sources and digesting them with\nlanguage models to generate structured outputs and representations of\nsimilarity. A) Illustration of Hamming code for error correction in data\ntransmission. Transmission of data (d) and parity (p) bits\nenables error correction via redundancy\n(https://en.wikipedia.org/wiki/Hamming_code). B) Diagram showing\nthe different sources of metadata information and how to bundle\nthem. Three independent resources – the electronic labnotebook,\nthe data-associated metadata, and the publication – are shown as\nredundant entries. An AI language model can be used to extract required\nand standardized data elements for verification, using codewords as a\nmeans of error correction analogous to error correction in\ncommunication. C) Heatmap display of similarities between\nsources by keyword. A Jupyter notebook using GPT-4 has been\nused to create a structured output in the form of a CSV file, (see Table\n1). The digestion of a labnotebook entry, a metadata file server file,\nand this manuscript are used to check for keywords. The consistency of\nthe keywords across the sources is displayed in a heatmap using the\ncosine distance for semantic similarity estimation <a href=\"https://spacy.io/api/doc\">(https://spacy.io/api/doc)</a>."
}
